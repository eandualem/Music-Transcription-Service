{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Karaoke Scoring System**\n",
    "\n",
    "### **Overview:**\n",
    "The Karaoke Scoring System is meticulously designed to evaluate a user's singing performance against an original track. Utilizing advanced audio processing techniques and alignment strategies, it delivers precise and insightful scoring, ensuring users gain deep insights into their performance.\n",
    "\n",
    "### **KaraokeData:**\n",
    "At the core of our system is the `KaraokeData` class, serving as the single access point for essential data for a particular song: the original singer's audio, the instrumental track, and synchronized lyrics. Beyond just storage, this class adeptly parses lyrics into a structured format, ensuring time-specific lyric extraction, which is paramount for aligning user feedback with distinct moments in the song.\n",
    "\n",
    "#### **Utilization Within KaraokeData:**\n",
    "- The **original singer's audio** sets the standard for user performance comparisons.\n",
    "- The **instrumental track** is instrumental in audio preprocessing, aiding in identifying and attenuating background noises.\n",
    "- **Synchronized lyrics** enhance the user experience, providing context to the feedback and ensuring precision in alignment.\n",
    "\n",
    "### **AudioPreprocessor:**\n",
    "The `AudioPreprocessor` class refines the user's audio through:\n",
    "1. **Normalization**: Adjusting the audio to have zero mean and unit variance.\n",
    "2. **Silence Trimming**: Removing any leading and trailing silences from the user's audio.\n",
    "3. **Spectral Gate**: Filtering out frequencies below a threshold, significantly reducing low-level noise.\n",
    "4. **Adaptive Noise Reduction**: Harnessing the instrumental track to pinpoint and eliminate background noise from the user's audio.\n",
    "5. **Voice Activity Detection (VAD)**: Spotting segments where the user is actively singing, ensuring the vocal's prominence over potential background disturbances.\n",
    "\n",
    "### **Scoring Mechanisms:**\n",
    "Our system leverages diverse metrics to deliver a well-rounded evaluation:\n",
    "1. **Linguistic Accuracy Score**: Employs Google's Speech Transcription service to transcribe the user's audio to text. This transcribed text is then matched with the original lyrics, determining pronunciation and word accuracy.\n",
    "2. **Amplitude Matching Score**: Utilizes Dynamic Time Warping (DTW) to compare amplitude profiles between the user's audio and the original.\n",
    "3. **Pitch Matching Score**: Investigates the fundamental frequency contours of both the user's and original audio, ensuring tonal alignment.\n",
    "4. **Rhythm Score**: Contrasts onset patterns between the user's performance and the original, assessing synchronization and timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_vis import AudioVis\n",
    "from pipeline import Pipeline\n",
    "from karaoke_data import KaraokeData\n",
    "from audio_scorer import AudioScorer\n",
    "from audio_preprocessor import AudioPreprocessor\n",
    "from google_speech import GoogleSpeechTranscription\n",
    "from whisper import WhisperSpeechTranscription\n",
    "\n",
    "av = AudioVis()\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Attempted': '../data/KaraokeData/42029.wav', 'Lyrics': '../data/KaraokeData/SongsLyrics/Lyrics/42029_ko_lyrics.csv', 'Track': '../data/KaraokeData/SongsLyrics/Track/42029.mp3', 'Original': '../data/KaraokeData/SongsLyrics/Voice/42029.mp3'}\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"../data/KaraokeData/\"\n",
    "lyrics_dir = os.path.join(base_dir, \"SongsLyrics\", \"Lyrics\")\n",
    "track_dir = os.path.join(base_dir, \"SongsLyrics\", \"Track\")\n",
    "voice_dir = os.path.join(base_dir, \"SongsLyrics\", \"Voice\")\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "def add_to_data_dict(directory, key):\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".wav\") or file.endswith(\".mp3\") or file.endswith(\"_ko_lyrics.csv\"):\n",
    "            if directory == voice_dir:\n",
    "                if \"voice_1\" in file:\n",
    "                    song_id = file.split('_')[0]\n",
    "                    key = \"Original\"\n",
    "                elif \"voice_2\" in file:\n",
    "                    song_id = file.split('_')[0]\n",
    "                    key = \"Original Second\"\n",
    "                else:\n",
    "                    song_id = os.path.splitext(file)[0]\n",
    "            elif directory == lyrics_dir:\n",
    "                song_id = file.split('_ko_lyrics.csv')[0]\n",
    "            else:\n",
    "                song_id = os.path.splitext(file)[0]\n",
    "\n",
    "            data_dict.setdefault(song_id, {})\n",
    "            data_dict[song_id][key] = os.path.join(directory, file)\n",
    "\n",
    "# Populate the dictionary\n",
    "add_to_data_dict(base_dir, \"Attempted\")\n",
    "add_to_data_dict(lyrics_dir, \"Lyrics\")\n",
    "add_to_data_dict(track_dir, \"Track\")\n",
    "add_to_data_dict(voice_dir, \"Original\")\n",
    "\n",
    "# Print a sample\n",
    "print(data_dict.get(\"42029\", {}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of id's with all files: 13\n",
      "['42029', '44924', '49032', '58659', '36520', '44957', '57730', '42113', '34302', '51837', '63172', '27256', '45000']\n"
     ]
    }
   ],
   "source": [
    "# Extract usable song IDs\n",
    "required_keys = {\"Attempted\", \"Lyrics\", \"Track\", \"Original\"}\n",
    "usable_ids = [song_id for song_id, data in data_dict.items() if required_keys.issubset(data.keys())]\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of id's with all files: {len(usable_ids)}\")\n",
    "print(usable_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0.        , 0.        , 0.        , ..., 0.9999695 , 0.952301  ,\n",
      "       0.90756226], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), '../data/KaraokeData/SongsLyrics/Lyrics/44957_ko_lyrics.csv', 44100)\n"
     ]
    }
   ],
   "source": [
    "def get_song_data(song_id):\n",
    "    \"\"\"Load audio data for a given song ID.\"\"\"\n",
    "\n",
    "    song_data = data_dict.get(song_id, {})\n",
    "    if \"Attempted\" in song_data and \"Lyrics\" in song_data and \"Track\" in song_data and (\"Original\" in song_data or \"Original Second\" in song_data):\n",
    "        original_audio, sr = librosa.load(song_data['Original'], sr=None, mono=True)\n",
    "        attempted_audio, sr = librosa.load(song_data['Attempted'], sr=None, mono=True)\n",
    "        track_audio, sr = librosa.load(song_data['Track'], sr=None, mono=True)\n",
    "        return original_audio, attempted_audio, track_audio, song_data['Lyrics'], sr\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Get the song data\n",
    "song_data = get_song_data(\"44957\")\n",
    "print(song_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_audio, attempted_audio, track_audio, raw_lyrics_data, sr = get_song_data(\"27256\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"./data/9951f8c7-2236-4997-89c1-19792e7a755e.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./data/f7ab11a9-946f-4cb0-9a63-aedef310f93e.png\" alt=\"Original Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./data/86dd55f8-a0ac-46c5-904f-81d3e7295e69.png\" alt=\"Original Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./data/483d0db5-8c56-47cb-9d59-dfb3cc2ff983.png\" alt=\"Original Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./data/01b47cce-1b51-4d36-bb11-c4512defd4af.png\" alt=\"Original Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./data/a3b10366-1982-4fed-9075-f6a5466c569e.png\" alt=\"Original Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"./data/3b933fcb-ad71-439a-9cbc-70154e399568.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./data/aab3408f-7569-4e37-ac3e-c518c4fbe201.png\" alt=\"Attempted Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./data/c87e5fad-2e58-42af-80f2-46aec69eb09d.png\" alt=\"Attempted Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./data/9c08ff9b-97d9-4d39-a578-7ca81a40d9a6.png\" alt=\"Attempted Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./data/7cd884b4-0069-4992-8ffe-b90a2bbec35c.png\" alt=\"Attempted Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./data/e9bd3e6d-2b0b-4045-9b5a-5ae7a9669458.png\" alt=\"Attempted Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"./data/5692e72e-f424-407e-9ce4-cb1372dc2f8e.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./data/bbe83f9f-e937-4e3a-97dd-948a8f8b6bef.png\" alt=\"Track Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./data/3efbefb9-d19e-4630-970d-3c1fd41a43c0.png\" alt=\"Track Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./data/30de1590-2411-474c-8044-712b4f037071.png\" alt=\"Track Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./data/0380c59a-f436-417c-ba01-1fcc8ba83ace.png\" alt=\"Track Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./data/ebd1bea1-51f3-4d59-a018-853d0b8c33f8.png\" alt=\"Track Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% skip_cell\n",
    "\n",
    "av.play_audio(original_audio, sr)\n",
    "av.wav_plot(original_audio, sr, title=\"Original Audio\")\n",
    "av.plot_spectrogram(original_audio, sr, title=\"Original Audio\")\n",
    "av.plot_log_spectrogram(original_audio, sr, title=\"Original Audio\")\n",
    "av.plot_mfcc(original_audio, sr, title=\"Original Audio\")\n",
    "av.plot_psd(original_audio, sr, title=\"Original Audio\")\n",
    "\n",
    "av.play_audio(attempted_audio, sr)\n",
    "av.wav_plot(attempted_audio, sr, title=\"Attempted Audio\")\n",
    "av.plot_spectrogram(attempted_audio, sr, title=\"Attempted Audio\")\n",
    "av.plot_log_spectrogram(attempted_audio, sr, title=\"Attempted Audio\")\n",
    "av.plot_mfcc(attempted_audio, sr, title=\"Attempted Audio\")\n",
    "av.plot_psd(attempted_audio, sr, title=\"Attempted Audio\")\n",
    "\n",
    "av.play_audio(track_audio, sr)\n",
    "av.wav_plot(track_audio, sr, title=\"Track Audio\")\n",
    "av.plot_spectrogram(track_audio, sr, title=\"Track Audio\")\n",
    "av.plot_log_spectrogram(track_audio, sr, title=\"Track Audio\")\n",
    "av.plot_mfcc(track_audio, sr, title=\"Track Audio\")\n",
    "av.plot_psd(track_audio, sr, title=\"Track Audio\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To simulate receiving audio in chunks, I have created split_into_chunks\n",
    "def split_into_chunks(audio, num_chunks=5):\n",
    "    \"\"\"Splits the audio data into a specified number of chunks.\"\"\"\n",
    "    chunk_size = len(audio) // num_chunks\n",
    "    chunks = [audio[i:i + chunk_size] for i in range(0, len(audio), chunk_size)]\n",
    "    return chunks[:num_chunks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KaraokeData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KaraokeData\n",
    "karaoke_data = KaraokeData(original_audio=original_audio, track_audio=track_audio, raw_lyrics_data=raw_lyrics_data, sampling_rate=sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_into_chunks(attempted_audio, 10)\n",
    "chunk = chunks[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Lyrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 13.76, 'lyrics': 'Ha'}\n",
      "{'time': 14.16, 'lyrics': 'ppi'}\n",
      "{'time': 14.36, 'lyrics': 'ness'}\n",
      "{'time': 15.56, 'lyrics': 'hit'}\n",
      "{'time': 15.96, 'lyrics': 'her'}\n",
      "{'time': 17.16, 'lyrics': 'like'}\n",
      "{'time': 17.55, 'lyrics': 'a'}\n",
      "{'time': 17.96, 'lyrics': 'train'}\n",
      "{'time': 18.76, 'lyrics': 'on'}\n",
      "{'time': 18.96, 'lyrics': 'a'}\n"
     ]
    }
   ],
   "source": [
    "parsed_lyrics = karaoke_data.lyrics_data\n",
    "for i in range(10):\n",
    "  print(parsed_lyrics[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position after start alignment: 0\n"
     ]
    }
   ],
   "source": [
    "karaoke_data.reset_alignment()  # Resetting any prior alignments\n",
    "start_singing_position = karaoke_data.align_audio(chunk, method=\"start\")\n",
    "print(f\"Position after start alignment: {karaoke_data.current_position}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align Using Lyrics Data: This method uses the first entry in the parsed lyrics data to align the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position after lyrics data alignment: 606816\n"
     ]
    }
   ],
   "source": [
    "# %% skip_cell\n",
    "\n",
    "karaoke_data.reset_alignment()  # Resetting any prior alignments\n",
    "start_singing_position = karaoke_data.align_audio(chunk, method=\"lyrics_data\")\n",
    "print(f\"Position after lyrics data alignment: {karaoke_data.current_position}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align Using Onset Detection:\n",
    "This method aligns the audio by detecting onsets in both the original audio and the provided audio chunk. It then attempts to align the first onset of the chunk with the corresponding onset in the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position after onset detection alignment: 583680\n"
     ]
    }
   ],
   "source": [
    "# %% skip_cell\n",
    "\n",
    "karaoke_data.reset_alignment()  # Resetting any prior alignments\n",
    "start_singing_position = karaoke_data.align_audio(chunk, method=\"onset_detection\")\n",
    "print(f\"Position after onset detection alignment: {karaoke_data.current_position}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Segment Retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_length = len(chunk)  # Using the length of the first audio chunk\n",
    "retrieved_original_segment, retrieved_track_segment = karaoke_data.get_next_segment(segment_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"./data/67c31663-416f-4ca5-9e2a-cb995853b633.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./data/d1b5d7cc-a4ec-4e55-841c-fd29b1eae7f7.png\" alt=\"Chunk Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"./data/a9305871-c285-4364-8939-b30101680704.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./data/aef192b5-b672-414b-ab15-a4518d2532dd.png\" alt=\"Original Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"./data/c515e489-24cf-4d38-830d-e4dd1a4ce8c3.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./data/9f0dd7ea-08e3-4259-90e6-8b48ac269e4a.png\" alt=\"Track Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% skip_cell\n",
    "\n",
    "av.play_audio(chunk, sr)\n",
    "av.wav_plot(chunk, sr, title=\"Chunk Audio\")\n",
    "\n",
    "av.play_audio(retrieved_original_segment, sr)\n",
    "av.wav_plot(retrieved_original_segment, sr, title=\"Original Audio\")\n",
    "\n",
    "av.play_audio(retrieved_track_segment, sr)\n",
    "av.wav_plot(retrieved_track_segment, sr, title=\"Track Audio\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ha ppi ness hit her like a train on a track\n",
      "\n"
     ]
    }
   ],
   "source": [
    "segment_lyrics = karaoke_data.get_lyrics()\n",
    "print(segment_lyrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Audio Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = AudioPreprocessor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_effect(before, after, sr, effect_name, visualization_functions):\n",
    "    \"\"\"\n",
    "    Demonstrates the effect of a preprocessing function by playing and visualizing:\n",
    "    - The original audio\n",
    "    - The processed audio\n",
    "    - (Optional) The removed audio (difference between the original and processed audio)\n",
    "    - Visualizations specified in visualization_functions for each of the audios\n",
    "    \"\"\"\n",
    "    # Play original audio\n",
    "    print(f\"Original Audio ({effect_name}):\")\n",
    "    av.play_audio(before, sr)\n",
    "\n",
    "    # Play processed audio\n",
    "    print(f\"\\nTransformed Audio ({effect_name}):\")\n",
    "    av.play_audio(after, sr)\n",
    "\n",
    "    same_length = len(before) == len(after)\n",
    "\n",
    "    # If the lengths are the same, play the difference audio\n",
    "    if same_length:\n",
    "        difference = before - after\n",
    "        print(f\"\\nRemoved Audio ({effect_name}):\")\n",
    "        av.play_audio(difference, sr)\n",
    "\n",
    "    # Display visualizations\n",
    "    for viz_func in visualization_functions:\n",
    "        print(f\"\\nOriginal Audio - {effect_name}:\")\n",
    "        viz_func(before, sr)\n",
    "\n",
    "        print(f\"\\nTransformed Audio - {effect_name}:\")\n",
    "        viz_func(after, sr)\n",
    "\n",
    "        # If the lengths are the same, visualize the difference audio\n",
    "        if same_length:\n",
    "            print(f\"\\nDifference - {effect_name}:\")\n",
    "            viz_func(difference, sr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim Audio\n",
    "\n",
    "Description: Trimming silences involves removing any leading or trailing silent parts from an audio signal. This can be useful to eliminate unnecessary silent portions which don't contribute to the actual content.\n",
    "\n",
    "Implementation: The trim_audio function uses the librosa.effects.trim function to achieve this. The top_db parameter defines a threshold in decibels below which the audio is considered silent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Audio (Trimming):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/15c9c90c-fafc-499d-9acb-a22165599c59.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio (Trimming):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/e4bfdf45-b7c9-4f8e-8f06-9ef54361c378.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Audio - Trimming:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/d89b7f65-484e-477b-ac27-493f25a919fa.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio - Trimming:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/56762e6e-c301-457e-aa16-956abaa53fdc.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# vf = [av.wav_plot, av.plot_spectrogram, av.plot_mfcc]\n",
    "vf = [av.wav_plot]\n",
    "trimmed_chunk = ap.trim_audio(chunk)\n",
    "demonstrate_effect(chunk, trimmed_chunk, sr, \"Trimming\", vf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Audio\n",
    "\n",
    "Description: Normalization adjusts the audio amplitude so that its average amplitude is zero, and its standard deviation is one. This ensures that the audio's loudness is relatively consistent, which can be beneficial for further processing or analysis.\n",
    "\n",
    "Implementation: The _normalize_segment function subtracts the mean from the audio segment and then divides by the standard deviation. The normalize_audio function can normalize the entire audio or perform segment-wise normalization if a segment_length is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Audio (Normalization):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/40dfc4ab-111c-433d-8e1e-b5cc605b7375.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio (Normalization):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/104cf62c-57ed-4d31-8120-3b429fc79557.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Removed Audio (Normalization):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/a868ffa9-18f3-493b-bd26-ec63ee20b836.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Audio - Normalization:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/0ac30f43-1315-41be-8694-089ca849e992.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio - Normalization:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/6816c311-ea80-4ff8-aaeb-c3e9c13fa480.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Difference - Normalization:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/46306204-21b7-4bce-976f-87366a3ee2f3.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vf = [av.wav_plot]\n",
    "normalized_chunk = ap.normalize_audio(chunk)\n",
    "demonstrate_effect(chunk, normalized_chunk, sr, \"Normalization\", vf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Gate\n",
    "\n",
    "Description: This involves suppressing frequency components of the signal below a certain threshold. It helps in reducing noise or undesired frequencies.\n",
    "\n",
    "Implementation: In the spectral_gate function, an STFT (Short-Time Fourier Transform) is performed, and any frequencies below the threshold are set to zero. The processed signal is then reconstructed using the inverse STFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Audio (Spectral Gating):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/cf066b98-698b-4055-8f9f-89bb85ae4819.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio (Spectral Gating):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/6ebf6fd9-0678-49b9-893f-b630a5d29069.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Audio - Spectral Gating:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/135bd933-4df6-40b2-b4b2-94b2d705a708.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio - Spectral Gating:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/4fa086f9-7766-4c13-a015-b8a250060e06.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spectral_gated_chunk = ap.spectral_gate(chunk, threshold=0.1)\n",
    "demonstrate_effect(chunk, spectral_gated_chunk, sr, \"Spectral Gating\", vf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Noise Reduction\n",
    "\n",
    "Description: Adaptive noise reduction aims to reduce background noise from the user's audio using a reference (typically the instrumental track). By comparing the reference track with the user's audio, it identifies and subtracts common background elements, reducing interference or bleed from the instrumental.\n",
    "\n",
    "Implementation: In the given code, the method named spectral_masking is used for this purpose. It calculates a mask based on the ratio of magnitudes of the user audio to the combined magnitudes of the user and reference audios. This mask, when applied to the user's audio STFT, emphasizes the parts where the user's audio is dominant (like vocals) and suppresses the parts that are common with the reference (like instrumental bleed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Audio (Adaptive Noise Reduction):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/00eb5d6f-17b3-4d4e-afb1-2e4ba3bef668.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio (Adaptive Noise Reduction):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/8aa1ddba-cc7c-4247-8efb-5dd78149816b.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Audio - Adaptive Noise Reduction:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/679079f5-fa44-40a4-b373-ea405db86b26.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio - Adaptive Noise Reduction:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/27b06a08-8ef0-49f8-b0d2-5a0d86bd2714.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adaptively_reduced_chunk = ap.adaptive_noise_reduction(chunk, retrieved_track_segment, sr)\n",
    "demonstrate_effect(chunk, adaptively_reduced_chunk, sr, \"Adaptive Noise Reduction\", vf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voice Activity Detection\n",
    "\n",
    "Description: VAD is employed to detect when a person is speaking/singing in an audio clip. This is valuable when you want to separate or focus on vocal content and exclude long silences or background noise.\n",
    "\n",
    "Implementation: The voice_activity_detection function uses the librosa.effects.split function, which identifies segments of the signal that are above a certain loudness threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Audio (Voice Activity Detection):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/3dcf09af-baa2-413a-bc28-f7df0bcf2ccc.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio (Voice Activity Detection):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/c9586169-3e8d-47fd-b5bc-e0d1e980e57b.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Audio - Voice Activity Detection:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/9081bb54-5423-4f33-bba0-1d95511d46ad.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio - Voice Activity Detection:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/41346f03-7ce7-44bf-a43e-19dd8b5e9c51.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vad_chunk = ap.voice_activity_detection(chunk, sr, top_db=5)  # Adjust the top_db value as needed\n",
    "demonstrate_effect(chunk, vad_chunk, sr, \"Voice Activity Detection\", vf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description:\n",
    "Source separation aims to distinguish different sources within an audio signal. In this method, Harmonic/Percussive Source Separation (HPSS) is employed. HPSS works by analyzing the audio and determining which parts of the signal are steady (harmonic) and which parts are transient (percussive). This method is particularly useful for separating melodic content (like vocals or instrumental solos) from rhythmic content (like drums or percussions).\n",
    "\n",
    "Implementation:\n",
    "In the source_separation method, Librosa's hpss function is used to separate the harmonic and percussive components of the input audio chunk. The harmonic component, which corresponds to the melodic content, is returned, thus effectively filtering out the rhythmic or percussive elements of the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Audio (Source Separation):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/56256858-0885-4aa8-b850-93d2cac88248.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio (Source Separation):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/58a60d85-af94-4b8c-9300-5b6be165ad65.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Removed Audio (Source Separation):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/526dac4f-0c65-429d-906e-3e651549e62a.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Audio - Source Separation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/af9f1da7-a3d3-4ae4-9ddb-8714c94bb1e7.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio - Source Separation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/09056081-9dd7-419a-a489-a0459b6a322d.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Difference - Source Separation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/0914f4ae-bc34-474a-ab08-2e39a758c77c.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "source_separated_chunk = ap.source_separation(chunk, sr)\n",
    "demonstrate_effect(chunk, source_separated_chunk, sr, \"Source Separation\", vf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Masking\n",
    "\n",
    "Description: Spectral masking emphasizes certain frequency components based on a reference signal. This can help in reducing interference or background sounds.\n",
    "\n",
    "Implementation: The spectral_masking function calculates a mask based on the ratio of magnitudes of the user audio to the sum of magnitudes of the user and reference audios. This mask is then applied to the user's audio STFT, and the processed audio is reconstructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Audio (Spectral Masking):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/ea7b54bb-914d-4c72-86fa-f0f8e9268d3b.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio (Spectral Masking):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/73450d85-6bd0-4718-8992-8a7219d16d7f.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Audio - Spectral Masking:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/b2ec32d5-0779-48c7-9022-9c6ea1eecc26.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio - Spectral Masking:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/70f55980-3b9e-4f7a-9682-24fd743c99d4.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "masked_chunk = ap.spectral_masking(chunk, retrieved_track_segment)\n",
    "demonstrate_effect(chunk, masked_chunk, sr, \"Spectral Masking\", vf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Audio (normalize):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/2a61fb2e-95f9-4b76-b9ee-396515b87f87.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio (normalize):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/5da20584-06dc-4ff4-9292-1aad7a1f8481.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Removed Audio (normalize):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/3376910c-485a-4152-955e-463baf791f88.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Audio - normalize:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/ad9bc9c9-23ba-4b2d-a984-c931763d1f5a.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio - normalize:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/334a33bd-df57-43b3-9af8-a24dbb76a85d.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Difference - normalize:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/2f4f393e-2ccd-4606-92e7-90b51b88375a.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Audio (adaptive_noise_reduction -> normalize):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/8757e59d-81b0-4024-bb19-e2da2592a333.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio (adaptive_noise_reduction -> normalize):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/3c874ba3-f74b-404b-a909-90aa5e3366a3.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Audio - adaptive_noise_reduction -> normalize:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/c045865d-46b3-4a18-b3ce-0879b38ce9b9.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio - adaptive_noise_reduction -> normalize:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/e3a35997-6806-4180-b80b-67e6fe972c23.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Audio (adaptive_noise_reduction -> source_separation -> normalize):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/3faf3e83-6b6d-45b3-9d7d-32be3d99568d.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio (adaptive_noise_reduction -> source_separation -> normalize):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/95a79d69-46bf-46fc-90a3-235ad8c21b00.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Audio - adaptive_noise_reduction -> source_separation -> normalize:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/93530313-15a9-4691-b583-1ad10bd0762f.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio - adaptive_noise_reduction -> source_separation -> normalize:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/5ce23455-7550-4cf3-ae12-1405f2dd15c0.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def demonstrate_pipeline(audio_chunk, pipeline, sr, **kwargs):\n",
    "    \"\"\"Demonstrates the effect of a preprocessing pipeline.\"\"\"\n",
    "    processed_audio = AudioPreprocessor.preprocess_audio(audio_chunk, pipeline, **kwargs)\n",
    "    pipeline_name = \" -> \".join(pipeline)\n",
    "    vf = [av.wav_plot]\n",
    "    demonstrate_effect(audio_chunk, processed_audio, sr, pipeline_name, vf)\n",
    "\n",
    "# Define the pipelines\n",
    "pipeline_1 = [\"normalize\"]\n",
    "pipeline_2 = [\"adaptive_noise_reduction\", \"normalize\"]\n",
    "pipeline_3 = [\"adaptive_noise_reduction\", \"source_separation\", \"normalize\"]\n",
    "\n",
    "# Additional arguments for the pipelines\n",
    "pipeline_args = {\n",
    "    \"reference_audio\": retrieved_track_segment\n",
    "    # You can add more arguments for other steps here as required\n",
    "}\n",
    "\n",
    "# Apply and demonstrate each pipeline\n",
    "demonstrate_pipeline(chunk, pipeline_1, sr)\n",
    "demonstrate_pipeline(chunk, pipeline_2, sr, **pipeline_args)\n",
    "demonstrate_pipeline(chunk, pipeline_3, sr, **pipeline_args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioScorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linguistic Accuracy**: The transcription is used to determine how closely the song content matches the actual lyrics. This is a `qualitative measure`.\n",
    "\n",
    "**Amplitude, Pitch, and Rhythm Matching**: These are `quantitative measures`. They compare the user's sung audio features with the reference (original) audio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcriber = GoogleSpeechTranscription()\n",
    "transcriber = WhisperSpeechTranscription()\n",
    "\n",
    "#fastdtw is suppose to be much faster but has bug\n",
    "audio_scorer = AudioScorer(transcriber, 'dtaidistance_fast')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Linguistic Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ha ppi ness hit her like a train on a track\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(karaoke_data.get_lyrics())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Happiness, hit her, like a train on a track'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriber.transcribe(chunk, sr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem here is because the audio is long, for short audio this will work fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Linguistic accuracy computation failed: unsupported operand type(s) for *: 'NoneType' and 'int'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linguistic Accuracy Score: 0.00\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'sr': sr, 'actual_lyrics': segment_lyrics, 'from_file': True}\n",
    "linguistic_score = audio_scorer.linguistic_accuracy_score(chunk, kwargs=kwargs)\n",
    "print(f\"Linguistic Accuracy Score: {linguistic_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rhythm Score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: Rhythm score quantifies how closely the rhythm of a user's audio matches a reference audio. It can be computed using onset strength, which is a measure of the abruptness of sound changes.\n",
    "\n",
    "**Implementation**: It compute onset strength for both user audio and reference audio using the `librosa.onset.onset_strength` function. It then computes the Dynamic Time Warping (DTW) similarity between these onset strength sequences to generate a rhythm score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rhythm Score: 0.9905208911994058\n"
     ]
    }
   ],
   "source": [
    "rhythm_score = audio_scorer.rhythm_score(np.array(chunk), retrieved_original_segment)\n",
    "print(\"Rhythm Score:\", rhythm_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pitch Matching Score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: Pitch matching score assesses how closely the pitch contour of a user's audio aligns with that of a reference audio. Pitch contour is the variation of pitch over time.\n",
    "\n",
    "**Implementation**: Uses the `librosa.pyin` function to extract pitch sequences from the user audio and reference audio. It then computes the DTW similarity between these pitch sequences to yield the pitch matching score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pitch Matching Score: 0.6381186807369086\n"
     ]
    }
   ],
   "source": [
    "pitch_score = audio_scorer.pitch_matching_score(chunk, retrieved_original_segment)\n",
    "print(\"Pitch Matching Score:\", pitch_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amplitude Matching Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: Amplitude matching score evaluates how well the amplitude envelope of a user's audio matches that of a reference audio.\n",
    "\n",
    "**Implementation**: Flattens the multi-dimensional audio arrays to 1D using `numpy.flatten`, then computes the DTW similarity between these 1D amplitude sequences to derive the amplitude matching score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amplitude Matching Score: 0.9999227028007309\n"
     ]
    }
   ],
   "source": [
    "amplitude_score = audio_scorer.amplitude_matching_score(chunk, retrieved_original_segment, sr)\n",
    "print(\"Amplitude Matching Score:\", amplitude_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    \"linguistic_accuracy_score\": {\n",
    "        \"chunk\": [],\n",
    "        \"original\": []\n",
    "    },\n",
    "    \"linguistic_similarity_score\": {\n",
    "        \"chunk\": [],\n",
    "        \"original\": []\n",
    "    },\n",
    "    \"amplitude_score\": {\n",
    "        \"chunk\": [],\n",
    "        \"original\": []\n",
    "    },\n",
    "    \"pitch_score\": {\n",
    "        \"chunk\": [\"adaptive_noise_reduction\", \"spectral_gate\", \"normalize\"],\n",
    "        \"original\": [\"spectral_gate\", \"normalize\"]\n",
    "    },\n",
    "    \"rhythm_score\": {\n",
    "        \"chunk\": [\"adaptive_noise_reduction\", \"spectral_gate\", \"normalize\"],\n",
    "        \"original\": [\"spectral_gate\", \"normalize\"]\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Processing Audio Chunk 1 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n",
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: {'linguistic_accuracy_score': 0.8863636363636364, 'linguistic_similarity_score': 0.0, 'amplitude_score': 0.998670979538087, 'pitch_score': 0.15672890500563721, 'rhythm_score': 0.9939604878392587}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n",
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: {'linguistic_accuracy_score': 0.2753623188405797, 'linguistic_similarity_score': 0.0, 'amplitude_score': 0.9981458507023967, 'pitch_score': 0.32203726238644603, 'rhythm_score': 0.9853766069539406}\n",
      "\n",
      "\n",
      " Scores for Chunk at 0.00 seconds \n",
      "\n",
      " Linguistic Accuracy Score: 0.28\n",
      " Linguistic Similarity Score: 0.00\n",
      " Amplitude Score: 1.00\n",
      " Pitch Score: 0.32\n",
      " Rhythm Score: 0.99\n",
      "\n",
      "Feedback:  Hmm, your phrasing seems a bit different from the original. Listen closely to the original singer's style and try to emulate it! \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Processing Audio Chunk 4 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n",
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: {'linguistic_accuracy_score': 0.2928870292887029, 'linguistic_similarity_score': 0.2931937172774869, 'amplitude_score': 0.9986744156349543, 'pitch_score': 0.5956255615904297, 'rhythm_score': 0.9867140461146159}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n",
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: {'linguistic_accuracy_score': 0.24, 'linguistic_similarity_score': 0.0, 'amplitude_score': 0.9984067198028099, 'pitch_score': 0.36154666859041257, 'rhythm_score': 0.9765363606692578}\n",
      "\n",
      "\n",
      " Scores for Chunk at 0.00 seconds \n",
      "\n",
      " Linguistic Accuracy Score: 0.24\n",
      " Linguistic Similarity Score: 0.00\n",
      " Amplitude Score: 1.00\n",
      " Pitch Score: 0.36\n",
      " Rhythm Score: 0.98\n",
      "\n",
      "Feedback:  Hmm, your phrasing seems a bit different from the original. Listen closely to the original singer's style and try to emulate it! \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Processing Audio Chunk 7 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n",
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: {'linguistic_accuracy_score': 0.2417582417582418, 'linguistic_similarity_score': 0.23529411764705888, 'amplitude_score': 0.9985030640113264, 'pitch_score': 0.45221527411268797, 'rhythm_score': 0.9882244812059702}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n",
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: {'linguistic_accuracy_score': 0.1145374449339207, 'linguistic_similarity_score': 0.0, 'amplitude_score': 0.9985269962314716, 'pitch_score': 0.36015975084982005, 'rhythm_score': 0.9884078853177477}\n",
      "\n",
      "\n",
      " Scores for Chunk at 0.00 seconds \n",
      "\n",
      " Linguistic Accuracy Score: 0.11\n",
      " Linguistic Similarity Score: 0.00\n",
      " Amplitude Score: 1.00\n",
      " Pitch Score: 0.36\n",
      " Rhythm Score: 0.99\n",
      "\n",
      "Feedback:  Hmm, your phrasing seems a bit different from the original. Listen closely to the original singer's style and try to emulate it! \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Processing Audio Chunk 10 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Linguistic accuracy computation failed: division by zero\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb Cell 65\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y251sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m18\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y251sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m chunk \u001b[39m=\u001b[39m attempted_audio[i\u001b[39m*\u001b[39mchunk_size_samples:(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mchunk_size_samples]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y251sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m scores \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mprocess_and_score(chunk)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y251sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m scores, feedback \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39mprocess_and_score(chunk)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y251sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m30\u001b[39m)\n",
      "File \u001b[0;32m~/Workspace/Tenacious/Music-Transcription-Service/Scoring/pipeline.py:67\u001b[0m, in \u001b[0;36mPipeline.process_and_score\u001b[0;34m(self, audio_chunk)\u001b[0m\n\u001b[1;32m     64\u001b[0m processed_audio_chunk_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_audio(audio_chunk, \u001b[39m\"\u001b[39m\u001b[39mchunk\u001b[39m\u001b[39m\"\u001b[39m, reference_audio\u001b[39m=\u001b[39mreference_audio)\n\u001b[1;32m     65\u001b[0m processed_original_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_audio(original_segment, \u001b[39m\"\u001b[39m\u001b[39moriginal\u001b[39m\u001b[39m\"\u001b[39m, reference_audio\u001b[39m=\u001b[39mreference_audio)\n\u001b[0;32m---> 67\u001b[0m scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_scores(processed_audio_chunk_data, processed_original_data)\n\u001b[1;32m     68\u001b[0m feedback \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_feedback(scores)\n\u001b[1;32m     70\u001b[0m \u001b[39m# Update cumulative scores and chunk count\u001b[39;00m\n",
      "File \u001b[0;32m~/Workspace/Tenacious/Music-Transcription-Service/Scoring/pipeline.py:81\u001b[0m, in \u001b[0;36mPipeline._compute_scores\u001b[0;34m(self, processed_audio_chunk_data, processed_original_data)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_compute_scores\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m     78\u001b[0m                     processed_audio_chunk_data: Dict[\u001b[39mstr\u001b[39m, np\u001b[39m.\u001b[39marray],\n\u001b[1;32m     79\u001b[0m                     processed_original_data: Dict[\u001b[39mstr\u001b[39m, np\u001b[39m.\u001b[39marray]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mfloat\u001b[39m]:\n\u001b[1;32m     80\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute scores for processed audio data.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maudio_scorer\u001b[39m.\u001b[39;49mprocess_audio_chunk(\n\u001b[1;32m     82\u001b[0m         processed_audio_chunk_data,\n\u001b[1;32m     83\u001b[0m         processed_original_data,\n\u001b[1;32m     84\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkaraoke_data\u001b[39m.\u001b[39;49mget_lyrics(),\n\u001b[1;32m     85\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msr,\n\u001b[1;32m     86\u001b[0m         \u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     87\u001b[0m     )\n",
      "File \u001b[0;32m~/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_scorer.py:156\u001b[0m, in \u001b[0;36mAudioScorer.process_audio_chunk\u001b[0;34m(self, processed_audio_chunk_data, processed_original_data, actual_lyrics, sr, from_file)\u001b[0m\n\u001b[1;32m    149\u001b[0m     kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    150\u001b[0m         \u001b[39m'\u001b[39m\u001b[39msr\u001b[39m\u001b[39m'\u001b[39m: sr,\n\u001b[1;32m    151\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mactual_lyrics\u001b[39m\u001b[39m'\u001b[39m: actual_lyrics,\n\u001b[1;32m    152\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mreference_audio\u001b[39m\u001b[39m'\u001b[39m: processed_original_data[score_name],\n\u001b[1;32m    153\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mfrom_file\u001b[39m\u001b[39m'\u001b[39m: from_file\n\u001b[1;32m    154\u001b[0m     }\n\u001b[1;32m    155\u001b[0m     user_audio \u001b[39m=\u001b[39m processed_audio_chunk_data[score_name]\n\u001b[0;32m--> 156\u001b[0m     scores[score_name] \u001b[39m=\u001b[39m scoring_function(user_audio, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    157\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mScores: \u001b[39m\u001b[39m{\u001b[39;00mscores\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[39mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m~/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_scorer.py:107\u001b[0m, in \u001b[0;36mAudioScorer.linguistic_similarity_with_original\u001b[0;34m(self, user_audio, reference_audio, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[39mreturn\u001b[39;00m debug_info\n\u001b[1;32m    105\u001b[0m \u001b[39m# Collapsible debugging section\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[39m# self.av.display_collapsible(generate_debug_output(self))\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_levenshtein_similarity(user_transcription, original_transcription)\n",
      "File \u001b[0;32m~/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_scorer.py:38\u001b[0m, in \u001b[0;36mAudioScorer._levenshtein_similarity\u001b[0;34m(text1, text2)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute Levenshtein similarity between two strings.\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m distance \u001b[39m=\u001b[39m levenshtein_distance(text1\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mstrip(), text2\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mstrip())\n\u001b[0;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m (distance \u001b[39m/\u001b[39;49m \u001b[39mmax\u001b[39;49m(\u001b[39mlen\u001b[39;49m(text1), \u001b[39mlen\u001b[39;49m(text2)))\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "ids = ['27256', '58659']\n",
    "\n",
    "for song_id in ids:\n",
    "    original_audio, attempted_audio, track_audio, raw_lyrics_data, sr = get_song_data(song_id)\n",
    "\n",
    "    # Initialize the pipeline\n",
    "    pipeline = Pipeline(original_audio=original_audio, track_audio=track_audio, raw_lyrics_data=raw_lyrics_data, sr=sr, pipelines=pipelines)\n",
    "\n",
    "    # Assuming you want to process chunks of the attempted_audio\n",
    "    chunk_size_seconds = 20\n",
    "    chunk_size_samples = chunk_size_seconds * sr\n",
    "\n",
    "    # Split attempted_audio into chunks and process\n",
    "    for i in range(0, int(len(attempted_audio)/chunk_size_samples), 1):\n",
    "        if i % 3 != 0 and i != 0:\n",
    "            pipeline.karaoke_data.get_next_segment(chunk_size_samples)\n",
    "            continue\n",
    "\n",
    "        # ---------------------------------- Debugging ----------------------------------\n",
    "        print(\"\\n\" + \"\" + \"\" * 18 + \"\")\n",
    "        print(f\" Processing Audio Chunk {i+1} \")\n",
    "        print(\"\" + \"\" * 18 + \"\" + \"\\n\")\n",
    "\n",
    "\n",
    "        chunk = attempted_audio[i*chunk_size_samples:(i+1)*chunk_size_samples]\n",
    "        scores = pipeline.process_and_score(chunk)\n",
    "        scores, feedback = pipeline.process_and_score(chunk)\n",
    "        print(\"\\n\" + \"\" * 30)\n",
    "        print(f\" Scores for Chunk at {i/sr:.2f} seconds \")\n",
    "        print(\"\" + \"\" * 28 + \"\")\n",
    "        for score_type, score_value in scores.items():\n",
    "            print(f\" {score_type.replace('_', ' ').title()}: {score_value:.2f}\")\n",
    "        print(\"\\nFeedback: \" + feedback)\n",
    "        print(\"\" * 30 + \"\\n\")\n",
    "        print(\"\\n\" + \"\" * 40 + \"\\n\")\n",
    "        # ---------------------------------- Debugging ----------------------------------\n",
    "\n",
    "    # Get average scores\n",
    "    average_scores = pipeline.get_average_scores()\n",
    "    print(\"\\n\" + \"\" * 30)\n",
    "    print(f\" Average Scores for Song {song_id} \")\n",
    "    print(\"\" + \"\" * 28 + \"\")\n",
    "    for score_type, score_value in average_scores.items():\n",
    "        print(f\" {score_type.replace('_', ' ').title()}: {score_value:.2f}\")\n",
    "    print(\"\" * 30 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Processing Audio Chunk 1 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n",
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: {'linguistic_accuracy_score': 0.8863636363636364, 'linguistic_similarity_score': 0.0, 'amplitude_score': 0.9999448244110144, 'pitch_score': 0.15672890500563721, 'rhythm_score': 0.9939604878392587}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n",
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: {'linguistic_accuracy_score': 0.2753623188405797, 'linguistic_similarity_score': 0.0, 'amplitude_score': 0.9999054876534426, 'pitch_score': 0.32203726238644603, 'rhythm_score': 0.9853766069539406}\n",
      "\n",
      "\n",
      " Scores for Chunk at 0.00 seconds \n",
      "\n",
      " Linguistic Accuracy Score: 0.28\n",
      " Linguistic Similarity Score: 0.00\n",
      " Amplitude Score: 1.00\n",
      " Pitch Score: 0.32\n",
      " Rhythm Score: 0.99\n",
      "\n",
      "Feedback:  Hmm, your phrasing seems a bit different from the original. Listen closely to the original singer's style and try to emulate it! \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Processing Audio Chunk 4 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "CPUDispatcher(<function _viterbi at 0x16de41ea0>) returned a result with an exception set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/numba/core/serialize.py:30\u001b[0m, in \u001b[0;36m_numba_unpickle\u001b[0;34m(address, bytedata, hashed)\u001b[0m\n\u001b[1;32m     27\u001b[0m _unpickled_memo \u001b[39m=\u001b[39m {}\n\u001b[0;32m---> 30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numba_unpickle\u001b[39m(address, bytedata, hashed):\n\u001b[1;32m     31\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Used by `numba_unpickle` from _helperlib.c\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39m        unpickled object\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;31mSystemError\u001b[0m: _PyEval_EvalFrameDefault returned a result with an exception set",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb Cell 66\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m18\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m chunk \u001b[39m=\u001b[39m attempted_audio[i\u001b[39m*\u001b[39mchunk_size_samples:(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mchunk_size_samples]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m scores \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mprocess_and_score(chunk)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m scores, feedback \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39mprocess_and_score(chunk)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m30\u001b[39m)\n",
      "File \u001b[0;32m~/Workspace/Tenacious/Music-Transcription-Service/Scoring/pipeline.py:67\u001b[0m, in \u001b[0;36mPipeline.process_and_score\u001b[0;34m(self, audio_chunk)\u001b[0m\n\u001b[1;32m     64\u001b[0m processed_audio_chunk_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_audio(audio_chunk, \u001b[39m\"\u001b[39m\u001b[39mchunk\u001b[39m\u001b[39m\"\u001b[39m, reference_audio\u001b[39m=\u001b[39mreference_audio)\n\u001b[1;32m     65\u001b[0m processed_original_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_audio(original_segment, \u001b[39m\"\u001b[39m\u001b[39moriginal\u001b[39m\u001b[39m\"\u001b[39m, reference_audio\u001b[39m=\u001b[39mreference_audio)\n\u001b[0;32m---> 67\u001b[0m scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_scores(processed_audio_chunk_data, processed_original_data)\n\u001b[1;32m     68\u001b[0m feedback \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_feedback(scores)\n\u001b[1;32m     70\u001b[0m \u001b[39m# Update cumulative scores and chunk count\u001b[39;00m\n",
      "File \u001b[0;32m~/Workspace/Tenacious/Music-Transcription-Service/Scoring/pipeline.py:81\u001b[0m, in \u001b[0;36mPipeline._compute_scores\u001b[0;34m(self, processed_audio_chunk_data, processed_original_data)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_compute_scores\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m     78\u001b[0m                     processed_audio_chunk_data: Dict[\u001b[39mstr\u001b[39m, np\u001b[39m.\u001b[39marray],\n\u001b[1;32m     79\u001b[0m                     processed_original_data: Dict[\u001b[39mstr\u001b[39m, np\u001b[39m.\u001b[39marray]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mfloat\u001b[39m]:\n\u001b[1;32m     80\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute scores for processed audio data.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maudio_scorer\u001b[39m.\u001b[39;49mprocess_audio_chunk(\n\u001b[1;32m     82\u001b[0m         processed_audio_chunk_data,\n\u001b[1;32m     83\u001b[0m         processed_original_data,\n\u001b[1;32m     84\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkaraoke_data\u001b[39m.\u001b[39;49mget_lyrics(),\n\u001b[1;32m     85\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msr,\n\u001b[1;32m     86\u001b[0m         \u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     87\u001b[0m     )\n",
      "File \u001b[0;32m~/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_scorer.py:156\u001b[0m, in \u001b[0;36mAudioScorer.process_audio_chunk\u001b[0;34m(self, processed_audio_chunk_data, processed_original_data, actual_lyrics, sr, from_file)\u001b[0m\n\u001b[1;32m    149\u001b[0m     kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    150\u001b[0m         \u001b[39m'\u001b[39m\u001b[39msr\u001b[39m\u001b[39m'\u001b[39m: sr,\n\u001b[1;32m    151\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mactual_lyrics\u001b[39m\u001b[39m'\u001b[39m: actual_lyrics,\n\u001b[1;32m    152\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mreference_audio\u001b[39m\u001b[39m'\u001b[39m: processed_original_data[score_name],\n\u001b[1;32m    153\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mfrom_file\u001b[39m\u001b[39m'\u001b[39m: from_file\n\u001b[1;32m    154\u001b[0m     }\n\u001b[1;32m    155\u001b[0m     user_audio \u001b[39m=\u001b[39m processed_audio_chunk_data[score_name]\n\u001b[0;32m--> 156\u001b[0m     scores[score_name] \u001b[39m=\u001b[39m scoring_function(user_audio, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    157\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mScores: \u001b[39m\u001b[39m{\u001b[39;00mscores\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[39mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m~/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_scorer.py:124\u001b[0m, in \u001b[0;36mAudioScorer.pitch_matching_score\u001b[0;34m(self, user_audio, reference_audio, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpitch_matching_score\u001b[39m(\u001b[39mself\u001b[39m, user_audio: np\u001b[39m.\u001b[39mndarray, reference_audio: np\u001b[39m.\u001b[39mndarray, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[1;32m    123\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Pitch matching score.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m     user_pitch, _, _ \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39;49mpyin(user_audio, fmin\u001b[39m=\u001b[39;49mlibrosa\u001b[39m.\u001b[39;49mnote_to_hz(\u001b[39m\"\u001b[39;49m\u001b[39mC2\u001b[39;49m\u001b[39m\"\u001b[39;49m), fmax\u001b[39m=\u001b[39;49mlibrosa\u001b[39m.\u001b[39;49mnote_to_hz(\u001b[39m\"\u001b[39;49m\u001b[39mC7\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    125\u001b[0m     reference_pitch, _, _ \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39mpyin(\n\u001b[1;32m    126\u001b[0m         reference_audio, fmin\u001b[39m=\u001b[39mlibrosa\u001b[39m.\u001b[39mnote_to_hz(\u001b[39m\"\u001b[39m\u001b[39mC2\u001b[39m\u001b[39m\"\u001b[39m), fmax\u001b[39m=\u001b[39mlibrosa\u001b[39m.\u001b[39mnote_to_hz(\u001b[39m\"\u001b[39m\u001b[39mC7\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    128\u001b[0m     user_pitch \u001b[39m=\u001b[39m user_pitch[\u001b[39m~\u001b[39mnp\u001b[39m.\u001b[39misnan(user_pitch)]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/librosa/core/pitch.py:855\u001b[0m, in \u001b[0;36mpyin\u001b[0;34m(y, fmin, fmax, sr, frame_length, win_length, hop_length, n_thresholds, beta_parameters, boltzmann_parameter, resolution, max_transition_rate, switch_prob, no_trough_prob, fill_na, center, pad_mode)\u001b[0m\n\u001b[1;32m    852\u001b[0m p_init \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m n_pitch_bins)\n\u001b[1;32m    853\u001b[0m p_init[n_pitch_bins:] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m n_pitch_bins\n\u001b[0;32m--> 855\u001b[0m states \u001b[39m=\u001b[39m sequence\u001b[39m.\u001b[39;49mviterbi(observation_probs, transition, p_init\u001b[39m=\u001b[39;49mp_init)\n\u001b[1;32m    857\u001b[0m \u001b[39m# Find f0 corresponding to each decoded pitch bin.\u001b[39;00m\n\u001b[1;32m    858\u001b[0m freqs \u001b[39m=\u001b[39m fmin \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m (np\u001b[39m.\u001b[39marange(n_pitch_bins) \u001b[39m/\u001b[39m (\u001b[39m12\u001b[39m \u001b[39m*\u001b[39m n_bins_per_semitone))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/librosa/sequence.py:1316\u001b[0m, in \u001b[0;36mviterbi\u001b[0;34m(prob, transition, p_init, return_logp)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1311\u001b[0m     \u001b[39m# Vectorize the helper\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m     __viterbi \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvectorize(\n\u001b[1;32m   1313\u001b[0m         _helper, otypes\u001b[39m=\u001b[39m[np\u001b[39m.\u001b[39muint16, np\u001b[39m.\u001b[39mfloat64], signature\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(s,t)->(t),(1)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1314\u001b[0m     )\n\u001b[0;32m-> 1316\u001b[0m     states, logp \u001b[39m=\u001b[39m __viterbi(log_prob)\n\u001b[1;32m   1318\u001b[0m     \u001b[39m# Flatten out the trailing dimension introduced by vectorization\u001b[39;00m\n\u001b[1;32m   1319\u001b[0m     logp \u001b[39m=\u001b[39m logp[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/numpy/lib/function_base.py:2372\u001b[0m, in \u001b[0;36mvectorize.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2369\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_stage_2(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   2370\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[0;32m-> 2372\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_as_normal(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/numpy/lib/function_base.py:2365\u001b[0m, in \u001b[0;36mvectorize._call_as_normal\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2362\u001b[0m     vargs \u001b[39m=\u001b[39m [args[_i] \u001b[39mfor\u001b[39;00m _i \u001b[39min\u001b[39;00m inds]\n\u001b[1;32m   2363\u001b[0m     vargs\u001b[39m.\u001b[39mextend([kwargs[_n] \u001b[39mfor\u001b[39;00m _n \u001b[39min\u001b[39;00m names])\n\u001b[0;32m-> 2365\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_vectorize_call(func\u001b[39m=\u001b[39;49mfunc, args\u001b[39m=\u001b[39;49mvargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/numpy/lib/function_base.py:2446\u001b[0m, in \u001b[0;36mvectorize._vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2444\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Vectorized call to `func` over positional `args`.\"\"\"\u001b[39;00m\n\u001b[1;32m   2445\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2446\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_vectorize_call_with_signature(func, args)\n\u001b[1;32m   2447\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m   2448\u001b[0m     res \u001b[39m=\u001b[39m func()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/numpy/lib/function_base.py:2486\u001b[0m, in \u001b[0;36mvectorize._vectorize_call_with_signature\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2483\u001b[0m nout \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(output_core_dims)\n\u001b[1;32m   2485\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39mndindex(\u001b[39m*\u001b[39mbroadcast_shape):\n\u001b[0;32m-> 2486\u001b[0m     results \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49m(arg[index] \u001b[39mfor\u001b[39;49;00m arg \u001b[39min\u001b[39;49;00m args))\n\u001b[1;32m   2488\u001b[0m     n_results \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(results) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results, \u001b[39mtuple\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m   2490\u001b[0m     \u001b[39mif\u001b[39;00m nout \u001b[39m!=\u001b[39m n_results:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/librosa/sequence.py:1301\u001b[0m, in \u001b[0;36mviterbi.<locals>._helper\u001b[0;34m(lp)\u001b[0m\n\u001b[1;32m   1299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_helper\u001b[39m(lp):\n\u001b[1;32m   1300\u001b[0m     \u001b[39m# Transpose input\u001b[39;00m\n\u001b[0;32m-> 1301\u001b[0m     _state, logp \u001b[39m=\u001b[39m _viterbi(lp\u001b[39m.\u001b[39;49mT, log_trans, log_p_init)\n\u001b[1;32m   1302\u001b[0m     \u001b[39m# Transpose outputs for return\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m     \u001b[39mreturn\u001b[39;00m _state\u001b[39m.\u001b[39mT, logp\n",
      "\u001b[0;31mSystemError\u001b[0m: CPUDispatcher(<function _viterbi at 0x16de41ea0>) returned a result with an exception set"
     ]
    }
   ],
   "source": [
    "ids = ['27256', '58659']\n",
    "\n",
    "for song_id in ids:\n",
    "    original_audio, attempted_audio, track_audio, raw_lyrics_data, sr = get_song_data(song_id)\n",
    "\n",
    "    # Initialize the pipeline\n",
    "    pipeline = Pipeline(original_audio=original_audio, track_audio=track_audio, raw_lyrics_data=raw_lyrics_data, sr=sr, pipelines=pipelines)\n",
    "\n",
    "    # Assuming you want to process chunks of the attempted_audio\n",
    "    chunk_size_seconds = 20\n",
    "    chunk_size_samples = chunk_size_seconds * sr\n",
    "\n",
    "    # Split attempted_audio into chunks and process\n",
    "    for i in range(0, int(len(attempted_audio)/chunk_size_samples), 1):\n",
    "        if i % 3 != 0 and i != 0:\n",
    "            pipeline.karaoke_data.get_next_segment(chunk_size_samples)\n",
    "            continue\n",
    "\n",
    "        # ---------------------------------- Debugging ----------------------------------\n",
    "        print(\"\\n\" + \"\" + \"\" * 18 + \"\")\n",
    "        print(f\" Processing Audio Chunk {i+1} \")\n",
    "        print(\"\" + \"\" * 18 + \"\" + \"\\n\")\n",
    "\n",
    "\n",
    "        chunk = attempted_audio[i*chunk_size_samples:(i+1)*chunk_size_samples]\n",
    "        scores = pipeline.process_and_score(chunk)\n",
    "        scores, feedback = pipeline.process_and_score(chunk)\n",
    "        print(\"\\n\" + \"\" * 30)\n",
    "        print(f\" Scores for Chunk at {i/sr:.2f} seconds \")\n",
    "        print(\"\" + \"\" * 28 + \"\")\n",
    "        for score_type, score_value in scores.items():\n",
    "            print(f\" {score_type.replace('_', ' ').title()}: {score_value:.2f}\")\n",
    "        print(\"\\nFeedback: \" + feedback)\n",
    "        print(\"\" * 30 + \"\\n\")\n",
    "        print(\"\\n\" + \"\" * 40 + \"\\n\")\n",
    "        # ---------------------------------- Debugging ----------------------------------\n",
    "\n",
    "    # Get average scores\n",
    "    average_scores = pipeline.get_average_scores()\n",
    "    print(\"\\n\" + \"\" * 30)\n",
    "    print(f\" Average Scores for Song {song_id} \")\n",
    "    print(\"\" + \"\" * 28 + \"\")\n",
    "    for score_type, score_value in average_scores.items():\n",
    "        print(f\" {score_type.replace('_', ' ').title()}: {score_value:.2f}\")\n",
    "    print(\"\" * 30 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
