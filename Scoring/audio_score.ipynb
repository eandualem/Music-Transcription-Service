{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Karaoke Scoring System**\n",
    "\n",
    "### **Overview:**\n",
    "The Karaoke Scoring System is meticulously designed to evaluate a user's singing performance against an original track. Utilizing advanced audio processing techniques and alignment strategies, it delivers precise and insightful scoring, ensuring users gain deep insights into their performance.\n",
    "\n",
    "### **KaraokeData:**\n",
    "At the core of our system is the `KaraokeData` class, serving as the single access point for essential data for a particular song: the original singer's audio, the instrumental track, and synchronized lyrics. Beyond just storage, this class adeptly parses lyrics into a structured format, ensuring time-specific lyric extraction, which is paramount for aligning user feedback with distinct moments in the song.\n",
    "\n",
    "#### **Utilization Within KaraokeData:**\n",
    "- The **original singer's audio** sets the standard for user performance comparisons.\n",
    "- The **instrumental track** is instrumental in audio preprocessing, aiding in identifying and attenuating background noises.\n",
    "- **Synchronized lyrics** enhance the user experience, providing context to the feedback and ensuring precision in alignment.\n",
    "\n",
    "### **AudioPreprocessor:**\n",
    "The `AudioPreprocessor` class refines the user's audio through:\n",
    "1. **Normalization**: Adjusting the audio to have zero mean and unit variance.\n",
    "2. **Silence Trimming**: Removing any leading and trailing silences from the user's audio.\n",
    "3. **Spectral Gate**: Filtering out frequencies below a threshold, significantly reducing low-level noise.\n",
    "4. **Adaptive Noise Reduction**: Harnessing the instrumental track to pinpoint and eliminate background noise from the user's audio.\n",
    "5. **Voice Activity Detection (VAD)**: Spotting segments where the user is actively singing, ensuring the vocal's prominence over potential background disturbances.\n",
    "\n",
    "### **Scoring Mechanisms:**\n",
    "Our system leverages diverse metrics to deliver a well-rounded evaluation:\n",
    "1. **Linguistic Accuracy Score**: Employs Google's Speech Transcription service to transcribe the user's audio to text. This transcribed text is then matched with the original lyrics, determining pronunciation and word accuracy.\n",
    "2. **Amplitude Matching Score**: Utilizes Dynamic Time Warping (DTW) to compare amplitude profiles between the user's audio and the original.\n",
    "3. **Pitch Matching Score**: Investigates the fundamental frequency contours of both the user's and original audio, ensuring tonal alignment.\n",
    "4. **Rhythm Score**: Contrasts onset patterns between the user's performance and the original, assessing synchronization and timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_vis import AudioVis\n",
    "from karaoke_data import KaraokeData\n",
    "from audio_scorer import AudioScorer\n",
    "from audio_preprocessor import AudioPreprocessor\n",
    "from google_speech import GoogleSpeechTranscription\n",
    "\n",
    "av = AudioVis('../data/temp')\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Attempted': '../data/KaraokeData/42029.wav', 'Lyrics': '../data/KaraokeData/SongsLyrics/Lyrics/42029_ko_lyrics.csv', 'Track': '../data/KaraokeData/SongsLyrics/Track/42029.mp3', 'Original': '../data/KaraokeData/SongsLyrics/Voice/42029.mp3'}\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"../data/KaraokeData/\"\n",
    "lyrics_dir = os.path.join(base_dir, \"SongsLyrics\", \"Lyrics\")\n",
    "track_dir = os.path.join(base_dir, \"SongsLyrics\", \"Track\")\n",
    "voice_dir = os.path.join(base_dir, \"SongsLyrics\", \"Voice\")\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "def add_to_data_dict(directory, key):\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".wav\") or file.endswith(\".mp3\") or file.endswith(\"_ko_lyrics.csv\"):\n",
    "            if directory == voice_dir:\n",
    "                if \"voice_1\" in file:\n",
    "                    song_id = file.split('_')[0]\n",
    "                    key = \"Original\"\n",
    "                elif \"voice_2\" in file:\n",
    "                    song_id = file.split('_')[0]\n",
    "                    key = \"Original Second\"\n",
    "                else:\n",
    "                    song_id = os.path.splitext(file)[0]\n",
    "            elif directory == lyrics_dir:\n",
    "                song_id = file.split('_ko_lyrics.csv')[0]\n",
    "            else:\n",
    "                song_id = os.path.splitext(file)[0]\n",
    "\n",
    "            data_dict.setdefault(song_id, {})\n",
    "            data_dict[song_id][key] = os.path.join(directory, file)\n",
    "\n",
    "# Populate the dictionary\n",
    "add_to_data_dict(base_dir, \"Attempted\")\n",
    "add_to_data_dict(lyrics_dir, \"Lyrics\")\n",
    "add_to_data_dict(track_dir, \"Track\")\n",
    "add_to_data_dict(voice_dir, \"Original\")\n",
    "\n",
    "# Print a sample\n",
    "print(data_dict.get(\"42029\", {}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of id's with all files: 13\n",
      "['42029', '44924', '49032', '58659', '36520', '44957', '57730', '42113', '34302', '51837', '63172', '27256', '45000']\n"
     ]
    }
   ],
   "source": [
    "# Extract usable song IDs\n",
    "required_keys = {\"Attempted\", \"Lyrics\", \"Track\", \"Original\"}\n",
    "usable_ids = [song_id for song_id, data in data_dict.items() if required_keys.issubset(data.keys())]\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of id's with all files: {len(usable_ids)}\")\n",
    "print(usable_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0.        , 0.        , 0.        , ..., 0.9999695 , 0.952301  ,\n",
      "       0.90756226], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), '../data/KaraokeData/SongsLyrics/Lyrics/44957_ko_lyrics.csv', 44100)\n"
     ]
    }
   ],
   "source": [
    "def get_song_data(song_id):\n",
    "    \"\"\"Load audio data for a given song ID.\"\"\"\n",
    "\n",
    "    song_data = data_dict.get(song_id, {})\n",
    "    if \"Attempted\" in song_data and \"Lyrics\" in song_data and \"Track\" in song_data and (\"Original\" in song_data or \"Original Second\" in song_data):\n",
    "        original_audio, sr = librosa.load(song_data['Original'], sr=None, mono=True)\n",
    "        attempted_audio, sr = librosa.load(song_data['Attempted'], sr=None, mono=True)\n",
    "        track_audio, sr = librosa.load(song_data['Track'], sr=None, mono=True)\n",
    "        return original_audio, attempted_audio, track_audio, song_data['Lyrics'], sr\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Get the song data\n",
    "song_data = get_song_data(\"44957\")\n",
    "print(song_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_audio, attempted_audio, track_audio, raw_lyrics_data, sr = get_song_data(\"27256\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/b3249161-ae40-49b3-b130-f2b5c3ad788d.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/32b7bc5d-38de-4491-9ade-fe0c84017855.png\" alt=\"Original Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/903128a5-3df8-4e36-91b0-93a6af70f07f.png\" alt=\"Original Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/153a51a5-431e-419d-9404-84f1086bfae9.png\" alt=\"Original Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/9343e838-36e4-49f0-b607-7c6026f58346.png\" alt=\"Original Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/f3019db8-e34b-4976-b564-610d435f04da.png\" alt=\"Original Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/11fbb93b-201b-4945-874c-cdfb44a84524.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/efcca13b-04dd-4034-acce-3618e5e8e681.png\" alt=\"Attempted Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/cb8cd450-831f-4135-92ef-aea2f691f1c4.png\" alt=\"Attempted Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/de879761-2d18-4ef7-93da-f102819f9845.png\" alt=\"Attempted Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/70f81512-300b-47fb-ad72-2a4769507744.png\" alt=\"Attempted Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/89e840f4-1b6c-4381-ba4a-2f6673d80b67.png\" alt=\"Attempted Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/2febc384-7884-4bad-88e1-b3ee2f014bae.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/4b267f03-c40d-4b55-98d9-5d0dbcbcda2b.png\" alt=\"Track Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/4afc5df4-7c1f-4910-997c-3296391dc72f.png\" alt=\"Track Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/eed62798-c33b-471a-9c9f-22aee0e799f6.png\" alt=\"Track Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/a2176d47-9893-4d3a-b71b-657911395ec2.png\" alt=\"Track Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/edee8b07-c142-497c-bad7-91ab7dcc569d.png\" alt=\"Track Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% skip_cell\n",
    "\n",
    "av.play_audio(original_audio, sr)\n",
    "av.wav_plot(original_audio, sr, title=\"Original Audio\")\n",
    "av.plot_spectrogram(original_audio, sr, title=\"Original Audio\")\n",
    "av.plot_log_spectrogram(original_audio, sr, title=\"Original Audio\")\n",
    "av.plot_mfcc(original_audio, sr, title=\"Original Audio\")\n",
    "av.plot_psd(original_audio, sr, title=\"Original Audio\")\n",
    "\n",
    "av.play_audio(attempted_audio, sr)\n",
    "av.wav_plot(attempted_audio, sr, title=\"Attempted Audio\")\n",
    "av.plot_spectrogram(attempted_audio, sr, title=\"Attempted Audio\")\n",
    "av.plot_log_spectrogram(attempted_audio, sr, title=\"Attempted Audio\")\n",
    "av.plot_mfcc(attempted_audio, sr, title=\"Attempted Audio\")\n",
    "av.plot_psd(attempted_audio, sr, title=\"Attempted Audio\")\n",
    "\n",
    "av.play_audio(track_audio, sr)\n",
    "av.wav_plot(track_audio, sr, title=\"Track Audio\")\n",
    "av.plot_spectrogram(track_audio, sr, title=\"Track Audio\")\n",
    "av.plot_log_spectrogram(track_audio, sr, title=\"Track Audio\")\n",
    "av.plot_mfcc(track_audio, sr, title=\"Track Audio\")\n",
    "av.plot_psd(track_audio, sr, title=\"Track Audio\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To simulate receiving audio in chunks, I have created split_into_chunks\n",
    "def split_into_chunks(audio, num_chunks=5):\n",
    "    \"\"\"Splits the audio data into a specified number of chunks.\"\"\"\n",
    "    chunk_size = len(audio) // num_chunks\n",
    "    chunks = [audio[i:i + chunk_size] for i in range(0, len(audio), chunk_size)]\n",
    "    return chunks[:num_chunks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KaraokeData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KaraokeData\n",
    "karaoke_data = KaraokeData(original_audio=original_audio, track_audio=track_audio, raw_lyrics_data=raw_lyrics_data, sampling_rate=sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_into_chunks(attempted_audio, 10)\n",
    "chunk = chunks[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Lyrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 13.76, 'lyrics': 'Ha'}\n",
      "{'time': 14.16, 'lyrics': 'ppi'}\n",
      "{'time': 14.36, 'lyrics': 'ness'}\n",
      "{'time': 15.56, 'lyrics': 'hit'}\n",
      "{'time': 15.96, 'lyrics': 'her'}\n",
      "{'time': 17.16, 'lyrics': 'like'}\n",
      "{'time': 17.55, 'lyrics': 'a'}\n",
      "{'time': 17.96, 'lyrics': 'train'}\n",
      "{'time': 18.76, 'lyrics': 'on'}\n",
      "{'time': 18.96, 'lyrics': 'a'}\n"
     ]
    }
   ],
   "source": [
    "parsed_lyrics = karaoke_data.lyrics_data\n",
    "for i in range(10):\n",
    "  print(parsed_lyrics[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position after start alignment: 0\n"
     ]
    }
   ],
   "source": [
    "karaoke_data.reset_alignment()  # Resetting any prior alignments\n",
    "start_singing_position = karaoke_data.align_audio(chunk, method=\"start\")\n",
    "print(f\"Position after start alignment: {karaoke_data.current_position}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align Using Lyrics Data: This method uses the first entry in the parsed lyrics data to align the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position after lyrics data alignment: 606816\n"
     ]
    }
   ],
   "source": [
    "# %% skip_cell\n",
    "\n",
    "karaoke_data.reset_alignment()  # Resetting any prior alignments\n",
    "start_singing_position = karaoke_data.align_audio(chunk, method=\"lyrics_data\")\n",
    "print(f\"Position after lyrics data alignment: {karaoke_data.current_position}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align Using Onset Detection:\n",
    "This method aligns the audio by detecting onsets in both the original audio and the provided audio chunk. It then attempts to align the first onset of the chunk with the corresponding onset in the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position after onset detection alignment: 583680\n"
     ]
    }
   ],
   "source": [
    "# %% skip_cell\n",
    "\n",
    "karaoke_data.reset_alignment()  # Resetting any prior alignments\n",
    "start_singing_position = karaoke_data.align_audio(chunk, method=\"onset_detection\")\n",
    "print(f\"Position after onset detection alignment: {karaoke_data.current_position}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Segment Retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_length = len(chunk)  # Using the length of the first audio chunk\n",
    "retrieved_original_segment, retrieved_track_segment = karaoke_data.get_next_segment(segment_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/78a7975a-b8aa-41fa-8e94-895e7fe59fe8.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/c26ab894-b72e-4c0b-88bb-7effc787d96c.png\" alt=\"Chunk Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/3dd305fa-77c5-40d4-a31e-e9acbe59fe5d.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/7018cc4a-8a23-44ba-8212-c8509a4eaa7a.png\" alt=\"Original Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/224d865c-3082-46c3-aec4-238fe1e31f68.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/de022dea-f97c-4ea8-95b6-464853627841.png\" alt=\"Track Audio\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% skip_cell\n",
    "\n",
    "av.play_audio(chunk, sr)\n",
    "av.wav_plot(chunk, sr, title=\"Chunk Audio\")\n",
    "\n",
    "av.play_audio(retrieved_original_segment, sr)\n",
    "av.wav_plot(retrieved_original_segment, sr, title=\"Original Audio\")\n",
    "\n",
    "av.play_audio(retrieved_track_segment, sr)\n",
    "av.wav_plot(retrieved_track_segment, sr, title=\"Track Audio\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ha ppi ness hit her like a train on a track\n",
      "\n"
     ]
    }
   ],
   "source": [
    "segment_lyrics = karaoke_data.get_lyrics()\n",
    "print(segment_lyrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Audio Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = AudioPreprocessor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_effect(before, after, sr, effect_name, visualization_functions):\n",
    "    \"\"\"\n",
    "    Demonstrates the effect of a preprocessing function by playing and visualizing:\n",
    "    - The original audio\n",
    "    - The processed audio\n",
    "    - (Optional) The removed audio (difference between the original and processed audio)\n",
    "    - Visualizations specified in visualization_functions for each of the audios\n",
    "    \"\"\"\n",
    "    # Play original audio\n",
    "    print(f\"Original Audio ({effect_name}):\")\n",
    "    av.play_audio(before, sr)\n",
    "\n",
    "    # Play processed audio\n",
    "    print(f\"\\nTransformed Audio ({effect_name}):\")\n",
    "    av.play_audio(after, sr)\n",
    "\n",
    "    same_length = len(before) == len(after)\n",
    "\n",
    "    # If the lengths are the same, play the difference audio\n",
    "    if same_length:\n",
    "        difference = before - after\n",
    "        print(f\"\\nRemoved Audio ({effect_name}):\")\n",
    "        av.play_audio(difference, sr)\n",
    "\n",
    "    # Display visualizations\n",
    "    for viz_func in visualization_functions:\n",
    "        print(f\"\\nOriginal Audio - {effect_name}:\")\n",
    "        viz_func(before, sr)\n",
    "\n",
    "        print(f\"\\nTransformed Audio - {effect_name}:\")\n",
    "        viz_func(after, sr)\n",
    "\n",
    "        # If the lengths are the same, visualize the difference audio\n",
    "        if same_length:\n",
    "            print(f\"\\nDifference - {effect_name}:\")\n",
    "            viz_func(difference, sr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim Audio\n",
    "\n",
    "Description: Trimming silences involves removing any leading or trailing silent parts from an audio signal. This can be useful to eliminate unnecessary silent portions which don't contribute to the actual content.\n",
    "\n",
    "Implementation: The trim_audio function uses the librosa.effects.trim function to achieve this. The top_db parameter defines a threshold in decibels below which the audio is considered silent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Audio (Trimming):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/15c9c90c-fafc-499d-9acb-a22165599c59.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio (Trimming):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/e4bfdf45-b7c9-4f8e-8f06-9ef54361c378.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Audio - Trimming:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/d89b7f65-484e-477b-ac27-493f25a919fa.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio - Trimming:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/56762e6e-c301-457e-aa16-956abaa53fdc.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# vf = [av.wav_plot, av.plot_spectrogram, av.plot_mfcc]\n",
    "vf = [av.wav_plot]\n",
    "trimmed_chunk = ap.trim_audio(chunk)\n",
    "demonstrate_effect(chunk, trimmed_chunk, sr, \"Trimming\", vf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Audio\n",
    "\n",
    "Description: Normalization adjusts the audio amplitude so that its average amplitude is zero, and its standard deviation is one. This ensures that the audio's loudness is relatively consistent, which can be beneficial for further processing or analysis.\n",
    "\n",
    "Implementation: The _normalize_segment function subtracts the mean from the audio segment and then divides by the standard deviation. The normalize_audio function can normalize the entire audio or perform segment-wise normalization if a segment_length is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Audio (Normalization):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/c7013103-49a0-4bbd-bb17-ec0b9acbf58a.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio (Normalization):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/8c2edb5d-5618-4102-8f23-421c2f725d24.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Removed Audio (Normalization):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/1dd551bf-17a2-4c2e-843f-c7987d4d9afd.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Audio - Normalization:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/25522aea-09e9-4b05-96fa-4dd56a75ae24.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio - Normalization:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/08fccde2-49f6-44a4-a326-f6a837affe61.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Difference - Normalization:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/505622d2-6753-4de1-b94d-0f1f23140ca3.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vf = [av.wav_plot]\n",
    "normalized_chunk = ap.normalize_audio(chunk)\n",
    "demonstrate_effect(chunk, normalized_chunk, sr, \"Normalization\", vf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Gate\n",
    "\n",
    "Description: This involves suppressing frequency components of the signal below a certain threshold. It helps in reducing noise or undesired frequencies.\n",
    "\n",
    "Implementation: In the spectral_gate function, an STFT (Short-Time Fourier Transform) is performed, and any frequencies below the threshold are set to zero. The processed signal is then reconstructed using the inverse STFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Audio (Spectral Gating):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/cf066b98-698b-4055-8f9f-89bb85ae4819.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio (Spectral Gating):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/6ebf6fd9-0678-49b9-893f-b630a5d29069.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Audio - Spectral Gating:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/135bd933-4df6-40b2-b4b2-94b2d705a708.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio - Spectral Gating:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/4fa086f9-7766-4c13-a015-b8a250060e06.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spectral_gated_chunk = ap.spectral_gate(chunk, threshold=0.1)\n",
    "demonstrate_effect(chunk, spectral_gated_chunk, sr, \"Spectral Gating\", vf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Noise Reduction\n",
    "\n",
    "Description: Adaptive noise reduction aims to reduce background noise from the user's audio using a reference (typically the instrumental track). By comparing the reference track with the user's audio, it identifies and subtracts common background elements, reducing interference or bleed from the instrumental.\n",
    "\n",
    "Implementation: In the given code, the method named spectral_masking is used for this purpose. It calculates a mask based on the ratio of magnitudes of the user audio to the combined magnitudes of the user and reference audios. This mask, when applied to the user's audio STFT, emphasizes the parts where the user's audio is dominant (like vocals) and suppresses the parts that are common with the reference (like instrumental bleed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Audio (Adaptive Noise Reduction):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/00eb5d6f-17b3-4d4e-afb1-2e4ba3bef668.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio (Adaptive Noise Reduction):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/8aa1ddba-cc7c-4247-8efb-5dd78149816b.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Audio - Adaptive Noise Reduction:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/679079f5-fa44-40a4-b373-ea405db86b26.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio - Adaptive Noise Reduction:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/27b06a08-8ef0-49f8-b0d2-5a0d86bd2714.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adaptively_reduced_chunk = ap.adaptive_noise_reduction(chunk, retrieved_track_segment, sr)\n",
    "demonstrate_effect(chunk, adaptively_reduced_chunk, sr, \"Adaptive Noise Reduction\", vf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voice Activity Detection\n",
    "\n",
    "Description: VAD is employed to detect when a person is speaking/singing in an audio clip. This is valuable when you want to separate or focus on vocal content and exclude long silences or background noise.\n",
    "\n",
    "Implementation: The voice_activity_detection function uses the librosa.effects.split function, which identifies segments of the signal that are above a certain loudness threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Audio (Voice Activity Detection):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/3dcf09af-baa2-413a-bc28-f7df0bcf2ccc.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio (Voice Activity Detection):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/c9586169-3e8d-47fd-b5bc-e0d1e980e57b.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Audio - Voice Activity Detection:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/9081bb54-5423-4f33-bba0-1d95511d46ad.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio - Voice Activity Detection:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/41346f03-7ce7-44bf-a43e-19dd8b5e9c51.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vad_chunk = ap.voice_activity_detection(chunk, sr, top_db=5)  # Adjust the top_db value as needed\n",
    "demonstrate_effect(chunk, vad_chunk, sr, \"Voice Activity Detection\", vf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: Source separation is the process of separating the main audio source from the rest of the audio. The method used here employs Non-negative Matrix Factorization (NMF) on the Mel spectrogram of the audio chunk. NMF factorizes the spectrogram into two matrices: the components matrix and the activations matrix. Each row of the components matrix can be thought of as a \"template\" spectrum, and the corresponding row of the activations matrix tells when that template is active.\n",
    "\n",
    "Implementation: In the method source_separation, the code computes the Mel spectrogram of the input audio chunk, then performs NMF to get the components and activations. The main audio source is identified as the component with the highest sum of activations, and it is then synthesized back into the time domain to produce the separated main audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_separation(audio_chunk: np.array, sr: int = 22050) -> np.array:\n",
    "    \"\"\"Separates the harmonic component using Harmonic/Percussive source separation.\"\"\"\n",
    "    # Separate harmonic and percussive components\n",
    "    harmonic, _ = librosa.effects.hpss(audio_chunk)\n",
    "    return harmonic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Audio (Source Separation):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/6d260f84-365f-4119-93e9-84f502579e98.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio (Source Separation):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/7d685d7f-cf9e-4619-a533-4b934c30300d.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Removed Audio (Source Separation):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/ff6ba41b-69a7-4115-9925-aac0e7415867.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Audio - Source Separation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/ac248a72-33e7-49f7-ad82-d1bad84c751f.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio - Source Separation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/1236789b-3042-4ac2-ab16-0cff510e864e.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Difference - Source Separation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/b965444f-966f-4632-bf2b-a3dc7eb8b35b.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "source_separated_chunk = source_separation(chunk, sr)\n",
    "demonstrate_effect(chunk, source_separated_chunk, sr, \"Source Separation\", vf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Masking\n",
    "\n",
    "Description: Spectral masking emphasizes certain frequency components based on a reference signal. This can help in reducing interference or background sounds.\n",
    "\n",
    "Implementation: The spectral_masking function calculates a mask based on the ratio of magnitudes of the user audio to the sum of magnitudes of the user and reference audios. This mask is then applied to the user's audio STFT, and the processed audio is reconstructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Audio (Spectral Masking):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/ea7b54bb-914d-4c72-86fa-f0f8e9268d3b.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio (Spectral Masking):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/73450d85-6bd0-4718-8992-8a7219d16d7f.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Audio - Spectral Masking:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/b2ec32d5-0779-48c7-9022-9c6ea1eecc26.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio - Spectral Masking:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/70f55980-3b9e-4f7a-9682-24fd743c99d4.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "masked_chunk = ap.spectral_masking(chunk, retrieved_track_segment)\n",
    "demonstrate_effect(chunk, masked_chunk, sr, \"Spectral Masking\", vf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Audio (normalize):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/c4e0140f-850b-42ce-ada0-f6c2651d5b86.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio (normalize):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/0ff50452-b172-42e2-8001-a748311170cb.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Removed Audio (normalize):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/26aab21b-b2fb-4c78-8f46-038ebece90e4.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Audio - normalize:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/81d75914-ee44-4300-a692-93f9348776a1.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Audio - normalize:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/97a8adb7-9a5f-4283-ae25-aef4945e8bc5.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Difference - normalize:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/758510a6-13ed-4136-b708-9864772faa88.png\" alt=\"Audio Signal\" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ParameterError",
     "evalue": "Audio data must be of type numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParameterError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb Cell 46\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#X63sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Apply and demonstrate each pipeline\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#X63sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m demonstrate_pipeline(chunk, pipeline_1, sr)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#X63sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m demonstrate_pipeline(chunk, pipeline_2, sr, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpipeline_args)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#X63sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m demonstrate_pipeline(chunk, pipeline_3, sr, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpipeline_args)\n",
      "\u001b[1;32m/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb Cell 46\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdemonstrate_pipeline\u001b[39m(audio_chunk, pipeline, sr, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#X63sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Demonstrates the effect of a preprocessing pipeline.\"\"\"\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     processed_audio \u001b[39m=\u001b[39m AudioPreprocessor\u001b[39m.\u001b[39;49mpreprocess_audio(audio_chunk, pipeline, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#X63sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     pipeline_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m -> \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(pipeline)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#X63sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     vf \u001b[39m=\u001b[39m [av\u001b[39m.\u001b[39mwav_plot]\n",
      "File \u001b[0;32m~/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_preprocessor.py:122\u001b[0m, in \u001b[0;36mAudioPreprocessor.preprocess_audio\u001b[0;34m(audio, pipeline, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39min\u001b[39;00m processing_map:\n\u001b[1;32m    121\u001b[0m     \u001b[39mif\u001b[39;00m step \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39madaptive_noise_reduction\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 122\u001b[0m         audio \u001b[39m=\u001b[39m processing_map[step](audio, reference_audio\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mreference_audio\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    123\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m         audio \u001b[39m=\u001b[39m processing_map[step](audio)\n",
      "File \u001b[0;32m~/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_preprocessor.py:91\u001b[0m, in \u001b[0;36mAudioPreprocessor.adaptive_noise_reduction\u001b[0;34m(audio_chunk, reference_audio, sr)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39m# Compute the Short-Time Fourier Transform (STFT) of both audio signals\u001b[39;00m\n\u001b[1;32m     90\u001b[0m audio_stft \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39mstft(audio_chunk)\n\u001b[0;32m---> 91\u001b[0m reference_stft \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39;49mstft(reference_audio)\n\u001b[1;32m     93\u001b[0m \u001b[39m# Estimate the noise profile from the reference audio\u001b[39;00m\n\u001b[1;32m     94\u001b[0m noise_profile \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(np\u001b[39m.\u001b[39mabs(reference_stft), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/librosa/core/spectrum.py:230\u001b[0m, in \u001b[0;36mstft\u001b[0;34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode, out)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[39mraise\u001b[39;00m ParameterError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhop_length=\u001b[39m\u001b[39m{\u001b[39;00mhop_length\u001b[39m}\u001b[39;00m\u001b[39m must be a positive integer\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    229\u001b[0m \u001b[39m# Check audio is valid\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m util\u001b[39m.\u001b[39;49mvalid_audio(y, mono\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    232\u001b[0m fft_window \u001b[39m=\u001b[39m get_window(window, win_length, fftbins\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    234\u001b[0m \u001b[39m# Pad the window out to n_fft size\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/librosa/util/utils.py:295\u001b[0m, in \u001b[0;36mvalid_audio\u001b[0;34m(y, mono)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Determine whether a variable contains valid audio data.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \n\u001b[1;32m    242\u001b[0m \u001b[39mThe following conditions must be satisfied:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[39mnumpy.float32\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(y, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m--> 295\u001b[0m     \u001b[39mraise\u001b[39;00m ParameterError(\u001b[39m\"\u001b[39m\u001b[39mAudio data must be of type numpy.ndarray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    297\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(y\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39mfloating):\n\u001b[1;32m    298\u001b[0m     \u001b[39mraise\u001b[39;00m ParameterError(\u001b[39m\"\u001b[39m\u001b[39mAudio data must be floating-point\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mParameterError\u001b[0m: Audio data must be of type numpy.ndarray"
     ]
    }
   ],
   "source": [
    "def demonstrate_pipeline(audio_chunk, pipeline, sr, **kwargs):\n",
    "    \"\"\"Demonstrates the effect of a preprocessing pipeline.\"\"\"\n",
    "    processed_audio = AudioPreprocessor.preprocess_audio(audio_chunk, pipeline, **kwargs)\n",
    "    pipeline_name = \" -> \".join(pipeline)\n",
    "    vf = [av.wav_plot]\n",
    "    demonstrate_effect(audio_chunk, processed_audio, sr, pipeline_name, vf)\n",
    "\n",
    "# Define the pipelines\n",
    "pipeline_1 = [\"normalize\"]\n",
    "pipeline_2 = [\"adaptive_noise_reduction\", \"normalize\"]\n",
    "pipeline_3 = [\"adaptive_noise_reduction\", \"source_separation\", \"normalize\"]\n",
    "\n",
    "# Additional arguments for the pipelines\n",
    "pipeline_args = {\n",
    "    \"adaptive_noise_reduction\": {\"reference_audio\": retrieved_track_segment}\n",
    "}\n",
    "\n",
    "# Apply and demonstrate each pipeline\n",
    "demonstrate_pipeline(chunk, pipeline_1, sr)\n",
    "demonstrate_pipeline(chunk, pipeline_2, sr, **pipeline_args)\n",
    "demonstrate_pipeline(chunk, pipeline_3, sr, **pipeline_args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioScorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linguistic Accuracy**: The transcription is used to determine how closely the song content matches the actual lyrics. This is a `qualitative measure`.\n",
    "\n",
    "**Amplitude, Pitch, and Rhythm Matching**: These are `quantitative measures`. They compare the user's sung audio features with the reference (original) audio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber = GoogleSpeechTranscription()\n",
    "\n",
    "#fastdtw is suppose to be much faster but has bug\n",
    "audio_scorer = AudioScorer(transcriber, 'dtaidistance_fast')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Linguistic Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ha ppi ness hit her like a train on a track\n",
      "Com ing to wards her stuck still no turn ing back\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(karaoke_data.get_lyrics())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happiness hit her like a train on a try'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriber.transcribe(chunk, sr, from_file=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem here is because the audio is long, for short audio this will work fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"./data/e336ebe8-d326-4b0b-b0c4-e2ab04c04297.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transcription: happiness hit her like a train on a try\n",
      "actual_lyrics: Ha ppi ness hit her like a train on a track\n",
      "Com ing to wards her stuck still no turn ing back\n",
      "\n",
      "Linguistic Accuracy Score: 0.41\n"
     ]
    }
   ],
   "source": [
    "linguistic_score = audio_scorer.linguistic_accuracy_score(chunk, sr, segment_lyrics, from_file=True)\n",
    "print(f\"Linguistic Accuracy Score: {linguistic_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rhythm Score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: Rhythm score quantifies how closely the rhythm of a user's audio matches a reference audio. It can be computed using onset strength, which is a measure of the abruptness of sound changes.\n",
    "\n",
    "**Implementation**: It compute onset strength for both user audio and reference audio using the `librosa.onset.onset_strength` function. It then computes the Dynamic Time Warping (DTW) similarity between these onset strength sequences to generate a rhythm score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rhythm Score: 0.9905208911994058\n"
     ]
    }
   ],
   "source": [
    "rhythm_score = audio_scorer.rhythm_score(np.array(chunk), retrieved_original_segment)\n",
    "print(\"Rhythm Score:\", rhythm_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pitch Matching Score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: Pitch matching score assesses how closely the pitch contour of a user's audio aligns with that of a reference audio. Pitch contour is the variation of pitch over time.\n",
    "\n",
    "**Implementation**: Uses the `librosa.pyin` function to extract pitch sequences from the user audio and reference audio. It then computes the DTW similarity between these pitch sequences to yield the pitch matching score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pitch Matching Score: 0.6381186807369086\n"
     ]
    }
   ],
   "source": [
    "pitch_score = audio_scorer.pitch_matching_score(chunk, retrieved_original_segment)\n",
    "print(\"Pitch Matching Score:\", pitch_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amplitude Matching Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: Amplitude matching score evaluates how well the amplitude envelope of a user's audio matches that of a reference audio.\n",
    "\n",
    "**Implementation**: Flattens the multi-dimensional audio arrays to 1D using `numpy.flatten`, then computes the DTW similarity between these 1D amplitude sequences to derive the amplitude matching score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amplitude Matching Score: 0.9999227028007309\n"
     ]
    }
   ],
   "source": [
    "amplitude_score = audio_scorer.amplitude_matching_score(chunk, retrieved_original_segment, sr)\n",
    "print(\"Amplitude Matching Score:\", amplitude_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karaoke_data import KaraokeData\n",
    "from audio_scorer import AudioScorer\n",
    "from audio_preprocessor import AudioPreprocessor\n",
    "from google_speech import GoogleSpeechTranscription\n",
    "from typing import List, Dict, Union, Tuple, Callable\n",
    "\n",
    "class Pipeline:\n",
    "    def __init__(self,\n",
    "                 original_audio: np.array,\n",
    "                 track_audio: np.array,\n",
    "                 raw_lyrics_data: str,\n",
    "                 sr: int,\n",
    "                 pipelines: Dict[str, Dict[str, List[str]]]):\n",
    "\n",
    "        self.sr = sr\n",
    "        self.pipelines = pipelines\n",
    "\n",
    "        # Initialize components\n",
    "        self.ap = AudioPreprocessor()\n",
    "        self.audio_scorer = AudioScorer(GoogleSpeechTranscription(), 'dtaidistance_fast')\n",
    "        self.karaoke_data = self._initialize_karaoke_data(original_audio, track_audio, raw_lyrics_data, sr)\n",
    "\n",
    "        # Track scores and chunks\n",
    "        self._reset_scores()\n",
    "        self.initialized = False\n",
    "\n",
    "    def _initialize_karaoke_data(self,\n",
    "                                 original_audio: np.array,\n",
    "                                 track_audio: np.array,\n",
    "                                 raw_lyrics_data: str,\n",
    "                                 sr: int) -> KaraokeData:\n",
    "        \"\"\"Helper method to initialize the KaraokeData instance.\"\"\"\n",
    "        return KaraokeData(original_audio, track_audio, raw_lyrics_data, sr)\n",
    "\n",
    "    def _reset_scores(self):\n",
    "        \"\"\"Reset cumulative scores and chunk count.\"\"\"\n",
    "        self.cumulative_scores = {\n",
    "            \"linguistic_accuracy_score\": 0,\n",
    "            \"linguistic_similarity_score\": 0,\n",
    "            \"amplitude_score\": 0,\n",
    "            \"pitch_score\": 0,\n",
    "            \"rhythm_score\": 0,\n",
    "        }\n",
    "        self.chunk_count = 0\n",
    "\n",
    "    def _preprocess_audio(self, audio: np.array, audio_type: str) -> Dict[str, np.array]:\n",
    "        \"\"\"\n",
    "        Preprocess audio (either chunk or original) using the specified pipeline.\n",
    "\n",
    "        Parameters:\n",
    "        - audio (np.array): The audio data to preprocess.\n",
    "        - audio_type (str): The type of audio, either \"chunk\" or \"original\".\n",
    "\n",
    "        Returns:\n",
    "        - Dict[str, np.array]: A dictionary of preprocessed audio data for each score.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            score_name: self.ap.preprocess_audio(audio, pipeline[audio_type])\n",
    "            for score_name, pipeline in self.pipelines.items()\n",
    "        }\n",
    "\n",
    "    def process_and_score(self, audio_chunk: np.array) -> Dict[str, float]:\n",
    "        \"\"\"Process and score a single audio chunk.\"\"\"\n",
    "        if not self.initialized:\n",
    "            self.karaoke_data.align_audio(audio_chunk, method=\"start\")\n",
    "            self.initialized = True\n",
    "\n",
    "        original_segment, _ = self.karaoke_data.get_next_segment(len(audio_chunk))\n",
    "\n",
    "        # Process audio data\n",
    "        processed_audio_chunk_data = self._preprocess_audio(audio_chunk, \"chunk\")\n",
    "        processed_original_data = self._preprocess_audio(original_segment, \"original\")\n",
    "\n",
    "        scores = self._compute_scores(processed_audio_chunk_data, processed_original_data)\n",
    "\n",
    "        # Update cumulative scores and chunk count\n",
    "        for score_name, score_value in scores.items():\n",
    "            self.cumulative_scores[score_name] += score_value\n",
    "        self.chunk_count += 1\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def _compute_scores(self,\n",
    "                        processed_audio_chunk_data: Dict[str, np.array],\n",
    "                        processed_original_data: Dict[str, np.array]) -> Dict[str, float]:\n",
    "        \"\"\"Compute scores for processed audio data.\"\"\"\n",
    "        return self.audio_scorer.process_audio_chunk(\n",
    "            processed_audio_chunk_data,\n",
    "            processed_original_data,\n",
    "            self.karaoke_data.get_lyrics(),\n",
    "            self.sr,\n",
    "            True\n",
    "        )\n",
    "\n",
    "    def get_average_scores(self) -> Dict[str, float]:\n",
    "        \"\"\"Compute average scores based on processed audio chunks.\"\"\"\n",
    "        return {\n",
    "            score_name: score_value / self.chunk_count\n",
    "            for score_name, score_value in self.cumulative_scores.items()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    \"linguistic_accuracy_score\": {\n",
    "        \"chunk\": [],\n",
    "        \"original\": []\n",
    "    },\n",
    "    \"linguistic_similarity_score\": {\n",
    "        \"chunk\": [],\n",
    "        \"original\": []\n",
    "    },\n",
    "    \"amplitude_score\": {\n",
    "        \"chunk\": [\"adaptive_noise_reduction\", \"spectral_gate\", \"normalize\"],\n",
    "        \"original\": [\"spectral_gate\", \"normalize\"]\n",
    "    },\n",
    "    \"pitch_score\": {\n",
    "        \"chunk\": [\"adaptive_noise_reduction\", \"spectral_gate\", \"normalize\"],\n",
    "        \"original\": [\"spectral_gate\", \"normalize\"]\n",
    "    },\n",
    "    \"rhythm_score\": {\n",
    "        \"chunk\": [\"adaptive_noise_reduction\", \"spectral_gate\", \"normalize\"],\n",
    "        \"original\": [\"spectral_gate\", \"normalize\"]\n",
    "    },\n",
    "}\n",
    "\n",
    "pipelines = {\n",
    "    \"linguistic_accuracy_score\": {\n",
    "        \"chunk\": [],\n",
    "        \"original\": []\n",
    "    },\n",
    "    \"linguistic_similarity_score\": {\n",
    "        \"chunk\": [],\n",
    "        \"original\": []\n",
    "    },\n",
    "    \"amplitude_score\": {\n",
    "        \"chunk\":  [\"normalize\"],\n",
    "        \"original\": []\n",
    "    },\n",
    "    \"pitch_score\": {\n",
    "        \"chunk\": [],\n",
    "        \"original\": []\n",
    "    },\n",
    "    \"rhythm_score\": {\n",
    "        \"chunk\": [],\n",
    "        \"original\": []\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Processing Audio Chunk 1 \n",
      "\n",
      "\n",
      " Lyrics:\n",
      "Ha ppi ness hit her like a train on a track\n",
      "\n",
      "\n",
      " Playing User Audio... \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/5f5b0666-eb2f-4d8b-9049-db657b48cc1b.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " User Transcription:\n",
      "happiness hit her like a train on a try\n",
      " Playing Original Audio... \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/0e11e1d5-aed3-41a0-8004-68cbcdc0272b.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Original Transcription:\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/805a7b32-2051-481e-bcd1-ced2f865b46f.png\" alt=\"Original Audio - Waveform \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/ee17d66c-3c1b-4c6b-aa9e-ad77b42690d1.png\" alt=\"User Audio - Waveform \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/e6d4b70f-b435-49da-af72-93c68e697cfd.png\" alt=\"Original Audio - Spectrogram \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/03474cdd-e738-4032-a9ea-fcced192cba0.png\" alt=\"User Audio - Spectrogram \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/59e0e2b2-7434-4422-9976-6ea3625efcac.png\" alt=\"Original Audio - Log Spectrogram \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/31d3375f-2b2f-4339-a40a-a2963d478e5b.png\" alt=\"User Audio - Log Spectrogram \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/98ad191a-5cae-4253-b56a-bf7e087b1107.png\" alt=\"Original Audio - MFCC \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/62dab4b9-8c17-4d49-9313-5a8920e1f2d8.png\" alt=\"User Audio - MFCC \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/4de7979e-d65f-41d6-8977-308f8c7345d9.png\" alt=\"Original Audio - Power Spectral Density \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/72360117-5310-4181-89ec-6d14c1cf3c16.png\" alt=\"User Audio - Power Spectral Density \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n",
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: {'linguistic_accuracy_score': 0.8863636363636364, 'linguistic_similarity_score': 0.0, 'amplitude_score': 0.9985342822282915, 'pitch_score': 0.5066390576442037, 'rhythm_score': 0.9870065519415654}\n",
      "\n",
      "\n",
      " Scores for Chunk at 0.00 seconds \n",
      "\n",
      " Linguistic Accuracy Score: 0.89\n",
      " Linguistic Similarity Score: 0.00\n",
      " Amplitude Score: 1.00\n",
      " Pitch Score: 0.51\n",
      " Rhythm Score: 0.99\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Processing Audio Chunk 6 \n",
      "\n",
      "\n",
      " Lyrics:\n",
      "they come\n",
      "And I ne ver want ed a ny thing from you\n",
      "\n",
      "\n",
      " Playing User Audio... \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/dbf52820-6a7b-4523-94be-51bbcd83677a.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " User Transcription:\n",
      "anything for you\n",
      " Playing Original Audio... \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/a331b31b-6cf0-4422-a137-1b8b110d5026.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Original Transcription:\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/03b19be4-a38f-4b27-a385-731119b14877.png\" alt=\"Original Audio - Waveform \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/38c3c252-9d79-48b9-b684-595c573726cb.png\" alt=\"User Audio - Waveform \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/c1932634-2c4c-4932-97a5-0c657139d7d3.png\" alt=\"Original Audio - Spectrogram \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/77b72cbe-08fa-4d99-ab2d-8cdf771acf2f.png\" alt=\"User Audio - Spectrogram \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/0b6dbe87-f6a6-41ba-9b23-1fa34275ba19.png\" alt=\"Original Audio - Log Spectrogram \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/903efbba-e3cb-4a10-8682-8f1e9c305280.png\" alt=\"User Audio - Log Spectrogram \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/922f791f-c654-419e-91bb-32ac97596f0d.png\" alt=\"Original Audio - MFCC \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/4a86038b-9ad7-4f06-84da-41eeda96b677.png\" alt=\"User Audio - MFCC \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/10532c41-5fd1-446d-bdef-cb864ad480d3.png\" alt=\"Original Audio - Power Spectral Density \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/d90eec9a-ca25-44e9-9190-f9bb61177dd3.png\" alt=\"User Audio - Power Spectral Density \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n",
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: {'linguistic_accuracy_score': 0.3137254901960784, 'linguistic_similarity_score': 0.0, 'amplitude_score': 0.998623050855266, 'pitch_score': 0.47009366334318875, 'rhythm_score': 0.9928464232886912}\n",
      "\n",
      "\n",
      " Scores for Chunk at 0.00 seconds \n",
      "\n",
      " Linguistic Accuracy Score: 0.31\n",
      " Linguistic Similarity Score: 0.00\n",
      " Amplitude Score: 1.00\n",
      " Pitch Score: 0.47\n",
      " Rhythm Score: 0.99\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Processing Audio Chunk 11 \n",
      "\n",
      "\n",
      " Lyrics:\n",
      "o ver the dog days are done\n",
      "Can you hear the hors es?\n",
      "'Cos here they come\n",
      "Here they come\n",
      "\n",
      "\n",
      " Playing User Audio... \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/fd1de0b8-5f33-434e-bd3b-b8191410e615.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " User Transcription:\n",
      "father says I'm begging you\n",
      " Playing Original Audio... \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/7aa01e29-d0ee-4b6e-a2b9-476290c34a75.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Original Transcription:\n",
      "wrote the dog days and down Gary the worst thing as ghosts\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/cb073004-401f-460d-867b-34111efe8f0b.png\" alt=\"Original Audio - Waveform \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/492c88e8-a3a8-4f04-85a7-fc310f9bbe27.png\" alt=\"User Audio - Waveform \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/d1f5916c-70bc-465c-9f6b-a52265142ae6.png\" alt=\"Original Audio - Spectrogram \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/f85d194e-5c5f-48b5-bde2-8b0d07878385.png\" alt=\"User Audio - Spectrogram \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/0ab65d04-7ec4-47a5-9952-485b5cf3bf6b.png\" alt=\"Original Audio - Log Spectrogram \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/fc880c1f-2d61-4fb0-ade3-aed0a25dbcf2.png\" alt=\"User Audio - Log Spectrogram \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/16268c6c-374c-4a31-bb25-af1882cca332.png\" alt=\"Original Audio - MFCC \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/6e78b987-28bd-4510-94b4-b550c3a6cdfc.png\" alt=\"User Audio - MFCC \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/12991a6a-86ab-4b38-80f5-0be626b27445.png\" alt=\"Original Audio - Power Spectral Density \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/a6f7a11a-9b48-4782-b149-ddc272152176.png\" alt=\"User Audio - Power Spectral Density \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n",
      "WARNING:be.kuleuven.dtai.distance:C-library not available, using the Python version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: {'linguistic_accuracy_score': 0.1797752808988764, 'linguistic_similarity_score': 0.2586206896551724, 'amplitude_score': 0.9986507211013924, 'pitch_score': 0.5347428151482408, 'rhythm_score': 0.9914003735417095}\n",
      "\n",
      "\n",
      " Scores for Chunk at 0.00 seconds \n",
      "\n",
      " Linguistic Accuracy Score: 0.18\n",
      " Linguistic Similarity Score: 0.26\n",
      " Amplitude Score: 1.00\n",
      " Pitch Score: 0.53\n",
      " Rhythm Score: 0.99\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Average Scores for Song 27256 \n",
      "\n",
      " Linguistic Accuracy Score: 0.46\n",
      " Linguistic Similarity Score: 0.09\n",
      " Amplitude Score: 1.00\n",
      " Pitch Score: 0.50\n",
      " Rhythm Score: 0.99\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Processing Audio Chunk 1 \n",
      "\n",
      "\n",
      " Lyrics:\n",
      "\n",
      "\n",
      " Playing User Audio... \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/30256cfb-98e6-4d26-8ccb-536af3e7bc08.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Linguistic accuracy computation failed: division by zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " User Transcription:\n",
      "\n",
      " Playing Original Audio... \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<audio controls style=\"width: 100%\"><source src=\"../data/temp/7f0fea18-dfaa-4ad1-9b3c-c3ba1895e36c.wav\" type=\"audio/wav\"></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Original Transcription:\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/f0d7cf89-7837-4777-be47-4b6b1803cbc6.png\" alt=\"Original Audio - Waveform \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/58ef6368-23ef-4200-bcec-c16e16321a37.png\" alt=\"User Audio - Waveform \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/a9b52014-40b9-48d3-9f87-667ee1bbe7a6.png\" alt=\"Original Audio - Spectrogram \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/53fde500-0ac9-4baa-964e-15acbdca7aa8.png\" alt=\"User Audio - Spectrogram \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/e3a92cea-08ff-47bf-b2dd-b5ce6d2b97f8.png\" alt=\"Original Audio - Log Spectrogram \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/a8a3fa02-ca03-4235-92c2-916be0e8c99d.png\" alt=\"User Audio - Log Spectrogram \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/693b6c7c-e55a-4e28-997f-266acbb63da8.png\" alt=\"Original Audio - MFCC \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/c8573704-45cc-42d2-bd31-76721f4911e6.png\" alt=\"User Audio - MFCC \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/1250a69a-1ff2-4679-8dba-da88ed8e1e46.png\" alt=\"Original Audio - Power Spectral Density \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/temp/e0d92904-b715-46c8-bab2-9c8eb4e7b1a6.png\" alt=\"User Audio - Power Spectral Density \" width=\"100%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb Cell 68\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m18\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m chunk \u001b[39m=\u001b[39m attempted_audio[i\u001b[39m*\u001b[39mchunk_size_samples:(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mchunk_size_samples]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m scores \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mprocess_and_score(chunk)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m30\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Scores for Chunk at \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m/\u001b[39msr\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m seconds \u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb Cell 68\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m processed_audio_chunk_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_audio(audio_chunk, \u001b[39m\"\u001b[39m\u001b[39mchunk\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m processed_original_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_audio(original_segment, \u001b[39m\"\u001b[39m\u001b[39moriginal\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_scores(processed_audio_chunk_data, processed_original_data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m# Update cumulative scores and chunk count\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mfor\u001b[39;00m score_name, score_value \u001b[39min\u001b[39;00m scores\u001b[39m.\u001b[39mitems():\n",
      "\u001b[1;32m/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb Cell 68\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_compute_scores\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m                     processed_audio_chunk_data: Dict[\u001b[39mstr\u001b[39m, np\u001b[39m.\u001b[39marray],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m                     processed_original_data: Dict[\u001b[39mstr\u001b[39m, np\u001b[39m.\u001b[39marray]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mfloat\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute scores for processed audio data.\"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maudio_scorer\u001b[39m.\u001b[39;49mprocess_audio_chunk(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m         processed_audio_chunk_data,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m         processed_original_data,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkaraoke_data\u001b[39m.\u001b[39;49mget_lyrics(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msr,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m         \u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e729/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_score.ipynb#Y123sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_scorer.py:130\u001b[0m, in \u001b[0;36mAudioScorer.process_audio_chunk\u001b[0;34m(self, processed_audio_chunk_data, processed_original_data, actual_lyrics, sr, from_file)\u001b[0m\n\u001b[1;32m    123\u001b[0m     kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    124\u001b[0m         \u001b[39m'\u001b[39m\u001b[39msr\u001b[39m\u001b[39m'\u001b[39m: sr,\n\u001b[1;32m    125\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mactual_lyrics\u001b[39m\u001b[39m'\u001b[39m: actual_lyrics,\n\u001b[1;32m    126\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mreference_audio\u001b[39m\u001b[39m'\u001b[39m: processed_original_data[score_name],\n\u001b[1;32m    127\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mfrom_file\u001b[39m\u001b[39m'\u001b[39m: from_file\n\u001b[1;32m    128\u001b[0m     }\n\u001b[1;32m    129\u001b[0m     user_audio \u001b[39m=\u001b[39m processed_audio_chunk_data[score_name]\n\u001b[0;32m--> 130\u001b[0m     scores[score_name] \u001b[39m=\u001b[39m scoring_function(user_audio, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    131\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mScores: \u001b[39m\u001b[39m{\u001b[39;00mscores\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    132\u001b[0m \u001b[39mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m~/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_scorer.py:82\u001b[0m, in \u001b[0;36mAudioScorer.linguistic_similarity_with_original\u001b[0;34m(self, user_audio, reference_audio, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m             plot_func(user_audio, sr, title\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUser Audio - \u001b[39m\u001b[39m{\u001b[39;00mtitle\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[39m# ---------------------------------- Debugging ----------------------------------\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_levenshtein_similarity(user_transcription, original_transcription)\n",
      "File \u001b[0;32m~/Workspace/Tenacious/Music-Transcription-Service/Scoring/audio_scorer.py:30\u001b[0m, in \u001b[0;36mAudioScorer._levenshtein_similarity\u001b[0;34m(text1, text2)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute Levenshtein similarity between two strings.\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m distance \u001b[39m=\u001b[39m levenshtein_distance(text1\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mstrip(), text2\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mstrip())\n\u001b[0;32m---> 30\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m (distance \u001b[39m/\u001b[39;49m \u001b[39mmax\u001b[39;49m(\u001b[39mlen\u001b[39;49m(text1), \u001b[39mlen\u001b[39;49m(text2)))\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "ids = ['27256', '58659']\n",
    "\n",
    "for song_id in ids:\n",
    "    original_audio, attempted_audio, track_audio, raw_lyrics_data, sr = get_song_data(song_id)\n",
    "\n",
    "    # Initialize the pipeline\n",
    "    pipeline = Pipeline(original_audio=original_audio, track_audio=track_audio, raw_lyrics_data=raw_lyrics_data, sr=sr, pipelines=pipelines)\n",
    "\n",
    "    # Assuming you want to process chunks of the attempted_audio\n",
    "    chunk_size_seconds = 20\n",
    "    chunk_size_samples = chunk_size_seconds * sr\n",
    "\n",
    "    # Split attempted_audio into chunks and process\n",
    "    for i in range(0, int(len(attempted_audio)/chunk_size_samples), 1):\n",
    "        if i % 5 != 0:\n",
    "            pipeline.karaoke_data.get_next_segment(chunk_size_samples)\n",
    "            continue\n",
    "\n",
    "        # ---------------------------------- Debugging ----------------------------------\n",
    "        print(\"\\n\" + \"\" + \"\" * 18 + \"\")\n",
    "        print(f\" Processing Audio Chunk {i+1} \")\n",
    "        print(\"\" + \"\" * 18 + \"\" + \"\\n\")\n",
    "\n",
    "\n",
    "        chunk = attempted_audio[i*chunk_size_samples:(i+1)*chunk_size_samples]\n",
    "        scores = pipeline.process_and_score(chunk)\n",
    "        print(\"\\n\" + \"\" * 30)\n",
    "        print(f\" Scores for Chunk at {i/sr:.2f} seconds \")\n",
    "        print(\"\" + \"\" * 28 + \"\")\n",
    "        for score_type, score_value in scores.items():\n",
    "            print(f\" {score_type.replace('_', ' ').title()}: {score_value:.2f}\")\n",
    "        print(\"\" * 30 + \"\\n\")\n",
    "\n",
    "        print(\"\\n\" + \"\" * 40 + \"\\n\")\n",
    "        # ---------------------------------- Debugging ----------------------------------\n",
    "\n",
    "    # Get average scores\n",
    "    average_scores = pipeline.get_average_scores()\n",
    "    print(\"\\n\" + \"\" * 30)\n",
    "    print(f\" Average Scores for Song {song_id} \")\n",
    "    print(\"\" + \"\" * 28 + \"\")\n",
    "    for score_type, score_value in average_scores.items():\n",
    "        print(f\" {score_type.replace('_', ' ').title()}: {score_value:.2f}\")\n",
    "    print(\"\" * 30 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
